{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Saturday, February 18, 2018\n",
    "# Connecting Spark 2 to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warehouse_location = abspath('spark-warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "     .builder \\\n",
    "     .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "     .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "     .enableHiveSupport() \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default field terminator in Hive is ^A. To type ^A, keep \"Control\" key pressed and type \"v\" followed by \"a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = \"CREATE TABLE IF NOT EXISTS sports (key INT, value STRING)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(s1)\n",
    "spark.sql('LOAD DATA LOCAL INPATH \"/Users/jhalverson/software/spark-2.2.1-bin-hadoop2.7/examples/src/main/resources/kv2.txt\" overwrite INTO TABLE sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "| 23| Jordan|\n",
      "| 99|Gretzky|\n",
      "| 77|Bourque|\n",
      "| 12|  Brady|\n",
      "| 33|   Bird|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sports\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was never able to set a custom field delimiter."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Wednesday, September 21, 2016  (Feb 2018 update)\n",
    "# Accumulators and broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NoApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a fast a...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = spark.read.text('text_file.md')\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcount = spark.sparkContext.accumulator(0)\n",
    "tcount = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countPython(line):\n",
    "     global pcount, tcount\n",
    "     if ('python' in line.value.lower()): pcount += 1\n",
    "     if ('the' in line.value.lower()): tcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines.foreach(lambda x: x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value=u'# Apache Spark')\n",
      "3 21\n"
     ]
    }
   ],
   "source": [
    "lines.foreach(countPython)\n",
    "print lines.first()\n",
    "print pcount.value, tcount.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"grep -ci 'The' text_file.md\" gives 21 and 3 in the case of 'Python'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to carry out an action such as first() on the RDD before evaluating the accumulators. This is because the transformation is not executed until the action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = ['code', 'hardware', 'causality', 'engine', 'computer', '# Apache Spark']\n",
    "ret = spark.sparkContext.broadcast(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         value|\n",
      "+--------------+\n",
      "|# Apache Spark|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.filter(lines.value.isin(keywords)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|high-level APIs i...|\n",
      "|Spark is built us...|\n",
      "|## Interactive Py...|\n",
      "|Alternatively, if...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.filter(lines.value.contains('Apache') | lines.value.contains('Python')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "out = lines.rdd.filter(lambda row: any([keyword in row.value for keyword in keywords]))\n",
    "print out.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'# Apache Spark', u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that']\n"
     ]
    }
   ],
   "source": [
    "print out.map(lambda u: u.value).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per partition basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 78]\n"
     ]
    }
   ],
   "source": [
    "lens = lines.rdd.map(lambda row: len(row.value))\n",
    "print lens.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   14|\n",
      "|    0|\n",
      "|   78|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "lines.withColumn('value', F.length(lines.value)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineCtrs(c1, c2):\n",
    "     return (c1[0] + c2[0], c1[1] + c2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partitionCounters(nums):\n",
    "     sumCount = [0, 0]\n",
    "     for num in nums:\n",
    "         sumCount[0] += num\n",
    "         sumCount[1] += 1\n",
    "     return [sumCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fastAvg(nums):\n",
    "     sumCount = nums.mapPartitions(partitionCounters).reduce(combineCtrs)\n",
    "     return sumCount[0] / float(sumCount[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.357894736842105"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastAvg(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative of using nums.map(lambda num: (num, 1)).reduce(combinCtrs) is slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.3578947368421"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = lens.map(lambda x: (x, 1))\n",
    "# pairs.mean() this line fails because not a numeric RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.3578947368\n"
     ]
    }
   ],
   "source": [
    "stats = lens.stats()\n",
    "mu = stats.mean()\n",
    "print mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=u'# Apache Spark'),\n",
       " Row(value=u'## Building Spark'),\n",
       " Row(value=u'    ./bin/pyspark'),\n",
       " Row(value=u'## Example Programs'),\n",
       " Row(value=u'## Running Tests'),\n",
       " Row(value=u'can be run using:'),\n",
       " Row(value=u'    ./dev/run-tests'),\n",
       " Row(value=u'## Configuration')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.rdd.filter(lambda row: len(row.value) > 0 and len(row.value) < 20).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'### Configuration'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.rdd.filter(lambda row: len(row.value) > 0 and len(row.value) < 20).reduce(lambda x, y: x[0] + y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

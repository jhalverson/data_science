{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Thursday, May 12, 2016\n",
    "# Blood donations competition in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rework our logistic regression model from scikit-learn in Spark. We begin by loading various MLlib modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are read in and then transformed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_lines = sc.textFile(\"/Users/jhalverson/data_science/project_blood_donations/train_blood.csv\")\n",
    "train_rdd = str_lines.mapPartitions(lambda x: csv.reader(x)).filter(lambda x: 'Months' not in x[1]).map(lambda x: map(int, x))\n",
    "str_lines = sc.textFile(\"/Users/jhalverson/data_science/project_blood_donations/test_blood.csv\")\n",
    "test_rdd = str_lines.mapPartitions(lambda x: csv.reader(x)).filter(lambda x: 'Months' not in x[1]).map(lambda x: map(int, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print out the first two records of the training set after the above transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[619, 2, 50, 12500, 98, 1], [664, 0, 13, 3250, 28, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the features are of equal importance, we standardize each one such that it has mean zero and variance unity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdsc = StandardScaler(withMean=True, withStd=True)\n",
    "model_sc = stdsc.fit(train_rdd)\n",
    "train_std = model_sc.transform(train_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create an RDD of LabeledPoints to input into the train method of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [2.0,50.0,98.0,0.510204081633]),\n",
       " LabeledPoint(1.0, [0.0,13.0,28.0,0.464285714286]),\n",
       " LabeledPoint(1.0, [1.0,16.0,35.0,0.457142857143]),\n",
       " LabeledPoint(1.0, [2.0,20.0,45.0,0.444444444444]),\n",
       " LabeledPoint(0.0, [1.0,24.0,77.0,0.311688311688])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = train_rdd.map(lambda x: LabeledPoint(x[5], np.asarray([x[1], x[2], x[4], float(x[2]) / x[4]])))\n",
    "trainData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(trainData, regParam=10)\n",
    "model.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47183995397390593"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([2, 20, 2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(659, 0.46747633699967134),\n",
       " (276, 0.454705458709579),\n",
       " (263, 0.4860667899742851)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = test_rdd.map(lambda x: (x[0], model.predict([x[1], x[2], x[4], float(x[2]) / x[4]])))\n",
    "testData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeLines(records):\n",
    "    for record in records:\n",
    "        return '%d,%.2f\\n' % (record[0], record[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = testData.map(lambda x: '%d,%.3f' % (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('halverson_logistic_regression_june3.dat', 'w')\n",
    "f.write(',Made Donation in March 2007\\n')\n",
    "for volunteer_id, prob in testData.collect():\n",
    "  f.write('%d,%.3f\\n' % (volunteer_id, prob))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

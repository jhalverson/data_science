{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Jonathan Halverson\n",
    "# October 12, 2016\n",
    "# DataFrames and machine learning in Spark 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a Spark DF to a Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    words|\n",
      "+---------+\n",
      "|[a, b, c]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[a, b, c]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words\n",
       "0  [a, b, c]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x = df.toPandas()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second row to the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    words|\n",
      "+---------+\n",
      "|[a, b, c]|\n",
      "|[e, f, g]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"e\", \"f\", \"g\"],)], [\"words\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=[u'a', u'b', u'c']), Row(words=[u'e', u'f', u'g'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"e\", \"f\", \"g\"],)], schema=[\"words\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|          vec|           resultVec|\n",
      "+-------------+--------------------+\n",
      "|[5.0,8.0,6.0]|[10.9696551146028...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import DCT\n",
    "\n",
    "df1 = spark.createDataFrame([(Vectors.dense([5.0, 8.0, 6.0]),)], [\"vec\"])\n",
    "dct = DCT(inverse=False, inputCol=\"vec\", outputCol=\"resultVec\")\n",
    "df2 = dct.transform(df1)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "ranges = sc.parallelize([[12, 45], [9, 11], [31, 122], [88, 109], [17, 61]])\n",
    "print type(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "| 12| 45|\n",
      "|  9| 11|\n",
      "| 31|122|\n",
      "| 88|109|\n",
      "| 17| 61|\n",
      "+---+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = ranges.toDF(schema=['a', 'b'])\n",
    "df.show()\n",
    "print type(df)\n",
    "print df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert an RDD of LabeledPoints to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([LabeledPoint(1.0, [1.0,6.0,7.0]), LabeledPoint(0.0, [12.0,2.0,9.0])],\n",
       " pyspark.rdd.RDD)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "lp = sc.parallelize([LabeledPoint(1, np.array([1, 6, 7])), LabeledPoint(0, np.array([12, 2, 9]))])\n",
    "lp.collect(), type(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "| [1.0,6.0,7.0]|  1.0|\n",
      "|[12.0,2.0,9.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------+\n",
      "|label|     features|  extra|\n",
      "+-----+-------------+-------+\n",
      "|    1|[7.0,2.0,9.0]|ignored|\n",
      "|    0|[6.0,3.0,1.0]|useless|\n",
      "+-----+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, Vectors.dense([7, 2, 9]), 'ignored'),\n",
    "                            (0, Vectors.dense([6, 3, 1]), 'useless')], [\"label\", \"features\", \"extra\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=1, features=DenseVector([7.0, 2.0, 9.0]), extra=u'ignored')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.133590225564,-0.460623607458,0.059944830801]\n",
      "Intercept: -0.00232930499821\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[There, will, be,...|(262144,[13007,89...|\n",
      "|    0|[I, will, run, ag...|(262144,[89356,10...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "tf = HashingTF(numFeatures=2**18, inputCol=\"words\", outputCol=\"features\")\n",
    "df = spark.createDataFrame([(1, ['There', 'will', 'be', 'cake'],), (0, ['I', 'will', 'run', 'again'],)], [\"label\", \"words\"])\n",
    "out = tf.transform(df)\n",
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(labels=1, words=[u'There', u'will', u'be', u'cake'], features=SparseVector(262144, {13007: 1.0, 89356: 1.0, 146559: 1.0, 167152: 1.0}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each line of the files into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham = sc.textFile('ham.txt')\n",
    "spam = sc.textFile('spam.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a map to the RDD and combine the RDD's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamLabelFeatures = ham.map(lambda email: [0, email.split()])\n",
    "spamLabelFeatures = spam.map(lambda email: [1, email.split()])\n",
    "trainRDD = hamLabelFeatures.union(spamLabelFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the RDD to a DataFrame and apply the hashing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[Dear, Spark, Lea...|(262144,[9639,142...|\n",
      "|    0|[Hi, Mom,, Apolog...|(262144,[1576,163...|\n",
      "|    0|[Wow,, hey, Fred,...|(262144,[18327,28...|\n",
      "|    0|[Hi, Spark, user,...|(262144,[15889,16...|\n",
      "|    0|[Thanks, Tom, for...|(262144,[8804,163...|\n",
      "|    0|[Good, job, yeste...|(262144,[14,25570...|\n",
      "|    0|[Summit, demo, go...|(262144,[31463,64...|\n",
      "|    1|[Dear, sir,, I, a...|(262144,[12781,36...|\n",
      "|    1|[Get, Viagra, rea...|(262144,[9129,261...|\n",
      "|    1|[Oh, my, gosh, yo...|(262144,[14,12946...|\n",
      "|    1|[YOUR, COMPUTER, ...|(262144,[24967,36...|\n",
      "|    1|[THIS, IS, NOT, A...|(262144,[14,12946...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF = trainRDD.toDF(schema=[\"label\", \"words\"])\n",
    "trainDF = tf.transform(trainDF)\n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (262144,[126466,140390,236986],[0.219230668165,-0.143429260905,0.647232520829])\n",
      "Intercept: -0.465236461182\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the outcome of a test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.createDataFrame([(['Fox', 'and', 'two', 'are', 'two', 'things'],)], [\"words\"])\n",
    "lrModel.transform(tf.transform(test)).select('prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

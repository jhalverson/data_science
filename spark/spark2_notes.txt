~/software/spark-2.0.0-bin-hadoop2.7/bin/pyspark

RDD is resilent distributed datasets. The data is automatically distributed across the cluster.

lines = sc.textFile('text_file.md')
lines.count()


### Wednesday, September 7, 2016
Spark can be used via the shell or as part of a standalone library.
Maven is an online package manager for Java applications. One
can specify the coordinates in the XML build file.

Hadoop MapReduce is batch while Spark is interactive and in-memory.
Spark runs on the Java Virtual Machine so Java 6 is needed.

pyspark is the Python Spark shell while spark-submit is the
binary which interprets flat Python scripts.

Pyspark automatically creates a SparkContext and it sets the enviromnet.
When using spark-submit one must create a SparkContext after importing
the appropriate modules. A SparkContext represents a connection to the
computing cluster. It be done in local mode which uses only one thread.

The driver and the executors. All work in Spark is expressed as making
new RDD's, transformation RDD's and carrying out actions. An RDD is
immutable and lazy. They are only evaluated when an action is called
such as writing out a file or computing a result. One can
use persist or cache to force the evaluation of an RDD.

Pair RDD's consist of tuples of key/values. The basic RDD method
can be used on them such as count, first, take.

Read chapter on I/O.

The files must be available at the same path on all nodes in
your cluster.

Some network filesystems like NFS, AFS are exposed to the user as
a regular filesystem.

Amazon S3 is fast when your compute nodes are located inside of
EC2 (elastic cloud compute). When you go over the net it is
not fast.

HDFS works great with Spark. It is designed to allow for nodes to
fail and keep going. It is a distributed filesystem.

Spark SQL is the SQL engine for structured data in Spark. The dataframe
is used in Spark SQL.

One can also load JSON using Spark SQL instead of using the json
reader.

Started reading Uncharted: Big data as a lens on human culture

Read the first half of chapter six on broadcast variables and accumulators.

Reviewed broadcast variables.

Read Chapter 7 on running on a cluster: The shell, local mode and then cluster mode
and client mode. Each process runs in a JVM. This includes the driver. The driver
interacts with the workers or executors via the cluster manager (Standalone [no
queuing], Apache Mesos, Hadoop YARN). To launch an application use
bin/spark-submit --master spark://host:7077 my_script.py
The Standalone cluster manager is a good choice if only using Spark
and queuing is not needed. YARN is in general the best choice because it
works seemlessly with HDFS. If external modules or libraries are needed
one must get those on the worker nodes. sbt is the Scala build tool.

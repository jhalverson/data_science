{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Saturday, April 30, 2016\n",
    "# Spark and IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('text_file.md', use_unicode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "py_lines = lines.filter(lambda line: 'Python' in line and 'Java' not in line)\n",
    "jv_lines = lines.filter(lambda line: 'Java' in line and 'Python' not in line)\n",
    "print py_lines.union(jv_lines).count(), lines.filter(lambda line: 'Python' in line or 'Java' in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RDDs in a Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalaFinder(object):\n",
    "    def __init__(self, keyword):\n",
    "        self.keyword = keyword\n",
    "    def printLines(self, RDD):\n",
    "        for line in RDD.collect():\n",
    "            if (self.keyword in line):\n",
    "                print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "and Spark Streaming for stream processing.\n",
      "and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).\n",
      "To build Spark and its example programs, run:\n",
      "Try the following command, which should return 1000:\n",
      "And run the following command, which should also return 1000:\n",
      "\"yarn\" to run on YARN, and \"local\" to run\n",
      "Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\n",
      "building for particular Hive and Hive Thriftserver distributions.\n"
     ]
    }
   ],
   "source": [
    "sf = ScalaFinder('and')\n",
    "sf.printLines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a histogram of the word count per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_chars = lines.map(lambda line: len(line))\n",
    "left, count = num_chars.histogram(range(0, 120, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10d2a2790>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGNCAYAAACMiXJiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4ZFV97vHvK6goKOIQcAbEqKgRVIwSAojD9WKuYwTj\ncNU4C8qNGnHA0BrnAQElRvRGNIjRmIgo1wQUTohxVsQEBFTAMLcjMorQv/vH3seurj7dXedQ51Sd\ns76f56nndK29atfaq6qr3tpr7b1TVUiSpHbdbNINkCRJk2UYkCSpcYYBSZIaZxiQJKlxhgFJkhpn\nGJAkqXGGAUmSGjexMJDkgCRnJLmiv301yb4Dy49Jsmbo9tVJtVeSpJVq8wk+94XAa4Af0oWS5wLH\nJ9mtqs4ACjgZePbAY65f6kZKkrTSTSwMVNUJQ0WHJHkp8DDgDCDA9VW1eskbJ0lSQ6ZizkCSzZI8\nHdgCOK0vLmCPJJcnOSfJ0UnuNLlWSpK0MmWS1yZI8kDga8AtgWuBP6uqE/tl+wNXA+cDOwBvATYD\nHlJVDhdIkjQmkw4DNwfuDmwNPA14OfDIqvr2HHXvDPwE2L+qPrukDZUkaQWb5ARCquq3wHn93dOT\n7AYcADxvjrqXJrkI2GmudSXx8ouSpKZUVcaxnqmYMzBgMzbQpn6+wF2BSzf88JqyG1TVVN0OPfTQ\nibdhudzsK/vJvrKfpvk2ThPbM5DkHcAXgIuA2wDPAPYCHpdkS+BNwGeAy4DtgbcDlwMOEUiSNEaT\nHCbYFjgW2A64gu5wwsdV1clJtgAeQHeOgdvR7Q04BfjTqrp6Qu2VJGlFmuR5BtabFzCw7DrgcUvY\nnGbsvffek27CsmFfjcZ+Gp19NRr7aelN9GiCceomEE7btmTs4zqSJAEkoVboBEJJkrTEDAOSJDXO\nMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAg\nSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS\n4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMM\nA5IkNc4wIElS4wwDkiQ1bmJhIMkBSc5IckV/+2qSfYfqrEpycZJrkpyaZOdJtVeSpJVqknsGLgRe\nA+wKPAQ4BTg+yYMAkhwMvBI4ENgNWA2cnGSryTRXkqSVKVU16Tb8TpKfA68FPgJcAhxZVW/vl21B\nFwheXVVHz/HYgunZlk6Ypv6VJK0cSaiqjGNdUzFnIMlmSZ4ObAGcBuwAbAucNFunqq7rl+0+kUZK\nkrRCbT7JJ0/yQOBrwC2Ba4H9quqcJLNf+JcPPWQ1cJclbKIkSSveRMMAcDbwB8DWwNOAf0jyyE08\nxv3ukiSN0UTDQFX9Fjivv3t6kt2AA4A392XbAhcNPGRb4LINr3HVwL/37m+SJC1/MzMzzMzMLMq6\np20C4SnAhVX1nCSXAO8fmkB4Od0Ewg/P8VgnEEqSmjHOCYQT2zOQ5B3AF+h++d8GeAawF/C4vsrh\nwOuTnA38EDgEuBI4bulbK0nSyjXJYYJtgWOB7YArgDOAx1XVyQBV9a4ktwKOArYBvg48tqqunlB7\nJUlakaZqmOCmcJhAktSSFXeeAUmSNDmGAUmSGmcYkCSpcYYBSZIaZxiQJKlxhgFJkhpnGJAkqXGG\nAUmSGmcYkCSpcYYBSZIaZxiQJKlxhgFJkhpnGJAkqXGGAUmSGmcYkCSpcYYBSZIaZxiQJKlxhgFJ\nkhpnGJAkqXGGAUmSGmcYkCSpcYYBSZIaZxiQJKlxhgFJkhpnGJAkqXGGAUmSGmcYkCSpcYYBSZIa\nZxiQJKlxhgFJkhpnGJAkqXGGAUmSGmcYkCSpcYYBSZIaZxiQJKlxhgFJkhpnGJAkqXETCwNJXpfk\nW0muSLI6yQlJ7j9U55gka4ZuX51UmyVJWokmuWdgL+ADwCOAfYAbgC8l2WagTgEnA9sN3PZd4nZK\nkrSibT6pJ66qxw3eT/Js4Apgd+DE2WLg+qpavcTNkySpGdM0Z+C2dO355UBZAXskuTzJOUmOTnKn\nyTRPkqSVKVU16TYAkOTTwL2Ah1bfqCT7A1cD5wM7AG8BNgMeUlXXDz2+uuwwTcK09K8kaWVJQlVl\nLOuahi+rJIcB+wF7VNUFG6l3Z+AnwP5V9dmhZYYBSVIzxhkGJjZnYFaS99EFgUduLAgAVNWlSS4C\ndpq7xqqBf+/d3yRJWv5mZmaYmZlZlHVPdM9AkiOAp9EFgXNGqH8n4CLg+VV17NAy9wxIkpqxIoYJ\nkhwFPAt4EvCDgUVXVtXVSbYE3gR8BrgM2B54O3BX4H5VdfXQ+gwDkqRmrJQwsIbu23t4Q1ZV1ZuT\nbAEcD+wK3A64FDgFeGNVXTzH+gwDkqRmrIgwMG6GAUlSS8YZBqbpPAOSJGkCDAOSJDXOMCBJUuMM\nA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOS\nJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1\nzjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4w\nIElS4wwDkiQ1zjAgSVLjJhYGkrwuybeSXJFkdZITktx/jnqrklyc5JokpybZeRLtlSRppZrknoG9\ngA8AjwD2AW4AvpRkm9kKSQ4GXgkcCOwGrAZOTrLV0jdXkqSVKVU16TYAkGRL4ArgiVV1YpIAlwBH\nVtXb+zpb0AWCV1fV0UOPL5iObVkrTEv/SpJWliRUVcaxrmmaM3Bbuvb8sr+/A7AtcNJshaq6DjgN\n2H3JWydJ0go1TWHgCOB04Gv9/e36v5cP1Vs9sEySJN1Em49aMcn5wEFVdcIGlv8J3S79HefbiCSH\n0f3a36NG26++gTqrBv69d3+TJGn5m5mZYWZmZlHWPfKcgSRrgGdV1XEbWP504LiqmtfehiTvA/YD\nHllV5w6U7wj8CNitqr4zUH4isLqqnje0HucMSJKaMa1zBn4PuGY+D0hyBLA/sM9gEOidD1wGPHag\n/hbAHsBXb1pTJUnSrI0OEyTZi+4QwNnk8ZQkO81R9Q7A04HvjfrESY4CngU8Cbgiyew8gCur6uqq\nqiSHA69PcjbwQ+AQ4Epgzr0TkiRp/jY6TJBkFfBXI67rR8Azq+pbIz1xN+xQrA0as1ZV1ZsH6h0K\nvBjYBvg6cEBVnTXH+hwmkCQ1Y5zDBJsKA1vTfQkDnAf8BfC5oWoFXFVVPx9HgxbKMCBJasmShYGh\nJ90bOKuqVo/jicfNMCBJaslEwsC0MwxIkloyzjAw8nkG+ie+J934/U50kwbXa0RV7TOOhkmSpKUx\nn5MO/U/geODmwFXAL+ao5s9gSZKWmfnMGfgecCe6Cwl9e1FbtQAOE0iSWjKpkw7dFzh8GoOAJEla\nuPmEgZ8Bv1mshkiSpMmYTxj4OPDUxWqIJEmajPnMGfh94GPAT+kuN3wecONwvar673E2cFTOGZAk\ntWRSJx1aM0K1qqrNblqTFsYwIElqyaTOM/DmTVeZum9jSZK0CZ6BcFG5Z0CStDgmdWihJElageZz\nBsI9R6lXVactvDmSJGmpjWsCYdFdp8AJhOtwmECStDgmNYHwzzfw+B2B5wEXAH87hjZJkqQlNHIY\nqKpjNrQsybuB7zLHVQwlSdJ0G8sEwqr6JfAR4C/HsT5JkrR0xnk0wa+Ae41xfZIkaQmMJQwkuRXw\nLOCycaxPkiQtnfkcWvhR5p6uf3tgd+COwGvG1C5JkrRExnFo4S+Ac4EPVNVx42rYfHlooSSpJRM5\ntLCqPFuhJEkrkF/wkiQ1bj4nHQIgydbAo4Ed+qLzgJOr6spxNkySJC2NeYWBJC8E3gtsNbToyiSv\nqqqPjK1lkiRpScxnAuETgOPp9gQcCZzVL9oZeDndaYmfXFUnLEI7R2mfEwglSc0Y5wTC+YSBr9Ad\nRviHw0MCSW4DfAP4RVXtMY6GzZdhQJLUknGGgflMIHwQcMxccwP6smOAXcbRKEmStHTmEwbCxn96\n+xNYkqRlaD7DBP8B3I5umOCqoWVb0Q0T/Kqq/mjsrRytfQ4TSJKaMZGTDgHvBv4Z+G6SI4Ez+/IH\n0E0g3Al4yjgaJUmSls7IewYAkrwMeBdw66FFVwOvqaoPjrFt8+KeAUlSSyZyNMHAk28DPIa1Jx36\nMd1Jh64YR4MWyjAgaVYyls/HReFngsZlomFgWhkGJM3qwsA0/t/zM0Hjs2SHFibZLMk7k7xkE/Ve\nmuTtSbzWgSRJy8ymvryfBfwl8O1N1Psm8BrgmeNolCRJWjqbCgP7AV+qqo2Ggar6DnAS8Iz5PHmS\nPZOckOSiJGuSPGdo+TF9+eDtq/N5DkmStHGbCgMPAU4ecV2nAg+e5/NvCXwfOAi4lvUH+ap//u0G\nbvvO8zkkSdJGbOo8A7cHVo+4rp8C28znyavqi8AXodsLMEeVANdX1ahtkCRJ87SpPQNXAncccV13\nAK7aZK35KWCPJJcnOSfJ0UnuNObnkCSpaZsKA2cBjx1xXY9m7VkJx+VfgGcD+wCvAh4GnJLkFmN+\nHkmSmrWpYYJ/Ag5L8qSqOn5DlZI8gS40vHKcjauqTw3cPTPJd4CfAI8HPrv+I1YN/Hvv/iZJ0vI3\nMzPDzMzMoqx7oycdSnJr4HRge+C9wNFVdcHA8h2AFwCvBs4Hdq2qaxfUkORK4ICq+vgm6p0HfLCq\n3j1U7kmHJAGedEhtWLILFVXVNUkeD3wBeC1wcJJf080luA2wdV/1HOBPFhoERtXPF7grcOliPo8k\nSS3Z5BkDq+pHwK50h/99BVgD3Ln/++99+YOr6sfzffIkWybZJckufVvu2d+/e7/sPUkenmT7JHsD\nJwCXM+cQgSRJWoiJXpug/4I/pb9bdIcSAhwDvAw4ni6I3I5ub8ApwBur6uI51uUwgSTAYQK1wQsV\nzcEwIGmWYUAtWLILFUmSpJXPMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMM\nA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOS\nJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1\nzjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zjAgSVLjJhoGkuyZ5IQkFyVZk+Q5c9RZ\nleTiJNckOTXJzpNoqyRJK9Wk9wxsCXwfOAi4FqjBhUkOBl4JHAjsBqwGTk6y1RK3U5KkFStVtela\nSyDJlcABVfXx/n6AS4Ajq+rtfdkWdIHg1VV19NDjayhLTIEwLf0rtaT7+JjG/3t+Jmh8klBVGce6\nJr1nYGN2ALYFTpotqKrrgNOA3SfVKEmSVpppDgPb9X8vHypfPbBMkiTdRNMcBjbG/WySJI3J5pNu\nwEZc1v/dFrhooHzbgWVDVg38e+/+JmkxdePzkhbbzMwMMzMzi7LuaZ9AeDHw/qEJhJfTTSD88NDj\nnUAoTcB0TtabxjaBnwkap3FOIJzonoEkWwL37u/eDLhnkl2An1fVhUkOB16f5Gzgh8AhwJXAcRNp\nsCRJK9BE9wwk2Rs4pb9bdHEe4Jiq+vO+zqHAi4FtgK/T7T04a451uWdAmgD3DMyHnwkan3HuGZia\nYYKbyjAgTYZhYD78TND4tHKeAUmStAQMA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMM\nA5IkNc4wIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNW7zSTdgpesu7zpdvISqJGmQYWDR\nTdsX7/SFE0nSZDlMIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wIElS4wwDkiQ1zpMO\nNWgaz4oInhlRkibFMNCkafzSnc6AIkktcJhAkqTGGQYkSWqcYUCSpMYZBiRJapxhQJKkxhkGJElq\nnGFAkqTGeZ4BaQM8OZOkVhgGpI2ati/e6QwokpY3hwkkSWqcYUCSpMZNdRhIsirJmqHbJZNulyRJ\nK8lymDNwNrD3wP0bJ9QOSZJWpOUQBm6sqtWTboQkSSvVVA8T9HZMcnGS85J8MskOk26QJEkrSab5\nmOUkjwO2ohsq2BY4BLgvcP+q+sVQ3ZrOw8Bs02gydcfPd+cZmK422U+jmsY2wTS+flq+klBVYzne\neKrDwLAktwbOB95RVe8bWmYYGMk0tgmm8UNyWr/k7KdRTGObYBpfPy1f4wwDy2HOwO9U1TVJzgR2\nmrvGqoF/78268w4lSVq+ZmZmmJmZWZR1L7c9A1vQ7Rk4qqreMrTMPQMjmcY2wTT+YprWX7z20yim\nsU0wja+flq9x7hmY6gmESd6TZM8kOyT5Q+AzwK2Aj024aZIkrRjTPkxwV+CTwB2BnwJfAx5eVRdO\ntFWSJK0gy2qYYGMcJhjVNLYJpnH36bTu/rafRjGNbYJpfP20fDUzTCBJkhafYUCSpMYZBiRJapxh\nQJKkxhkGJElq3LQfWqiGdLPStSn2k6RxMwxoikzbIVfT+qVrP0kaL4cJJElqnGFAkqTGGQYkSWqc\nYUCSpMYZBiRJapxhQJKkxhkGJElqnGFAkqTGGQYkSWqcYUCSpMYZBiRJapxhQJKkxhkGJElqnGFA\nkqTGGQYkSWqcYUCSpMYZBiRJatzmk26AJLUkyaSbsJ6qmnQTNGGGAUlaUtP2xTt94URLz2ECSZIa\nZxiQJKlxhgFJkhpnGJAkqXGGAUmSGmcYkCSpcYYBSZIa53kGJKlxnghJhgFJat60ffFOXzhZ6Rwm\nkCSpcYYBSZIatyzCQJKXJTk/ybVJvp1kj0m3SZKklWLqw0CS/YHDgbcAuwBfBb6Y5O4TbZgkSStE\npn3GZpJvAN+rqhcPlJ0LfKaqXj9QVtM5CcY2jWYa22WbRmObRjeN7ZrONk37d9M0SEJVjWW25VTv\nGUhyC+DBwElDi04Cdl/6FkmStPJMdRgA7ghsBlw+VL4a2G7pmyNJ0soz7WFAkiQtsmk/6dDPgBuB\nbYfKtwUuXb/6NJ6owjaNbhrbZZtGY5tGN43tmr42TeNZEVeyqQ4DVXV9ku8AjwX+aWDRY4B/HKrr\nO0eSpAWY6jDQOwz4+yTfpDus8CV08wX+dqKtkiRphZj6MFBVn05yB+AQ4M7AfwL7VtWFk22ZJEkr\nw9SfZ0CSJC2uZX80gacqXleS1yX5VpIrkqxOckKS+89Rb1WSi5Nck+TUJDtPor3TpO+7NUneP1Te\nfF8luXOSj/XvqWuTnJlkz6E69lOyeZK3JTmv76fzkvx1ks2G6jXVV0n27D+LLur/jz1njjob7ZMk\nt0zy/iQ/TXJVks8luevSbcXi21g/9e+tdyY5o9/+S5J8YvhsvAvtp2UdBjxV8Zz2Aj4APALYB7gB\n+FKSbWYrJDkYeCVwILAb3XkbTk6y1dI3dzokeTjwQuD7DJyOzb6CJLcD/oOuX/YF7kvXH6sH6jTf\nT73XAy8GXg7cBzgIeBnwutkKjfbVlnT/tw4CrmXolIcj9snhwFOApwN/DNwW+EKSZf09NmRj/bQl\nsCvd992uwBOBuwP/MhQ2F9ZPVbVsb8A3gA8NlZ0LvG3SbZuWW/8GugF4fH8/dIdlvm6gzhbAr4EX\nTbq9E+qjrYEf0QWpU4Ej7at1+udtwL9vZLn9tHa7Pw98dKjsY8Dn7avfbe+VwP+ez/un/z/6G+DP\nBurcje7Q88dOepuWop82UOd+wBrg/je1n5ZtovJUxSO7Ld0eoF/293egO0/D7/qtqq4DTqPdfjsa\n+Meq+jfWPeDavuo8Cfhmkk8luTzJ6UkOGFhuP631RWCfJPcB6Hd1PxI4sV9uX61vlD55CHDzoToX\nAT+g3X6D7ssf1n6+L7ifpv5ogo3wVMWjOQI4Hfhaf3+2b+bqt7ssVaOmRZIXAjsCz+iLBnfL2Ved\nHel2dR9Gt5dgV+D9/UVSjsJ++p2q+pskdwN+kOQGus/Yt1TV7KHQ9tX6RumT7YAbq+rnQ3UuZ/2T\n0jWh/0H8XuCEqrqkL15wPy3nMKBNSHIYXRrco/r9RZvQ1KEl/a+3t9L1z42zxYx2OraW+upmwDer\n6g39/TOS3Bs4ADhqE49tqZ9I8grgeXTjtWfSBacjklxQVX+3iYc31Vcjsk/mkGRz4Fi6Pb9/Mo51\nLtthAuZ9quK2JHkfsD+wT1VdMLDosv7vXP12GW15BN0epjOT/DbJb4E9gZcluZ7uPQb21SXAWUNl\nZwP36P/te2qtN9DNWfp0VZ1ZVcfS7VGZnUBoX61vlD65DNisP+fMoO1orN/6IPBJ4AHAo6rqlwOL\nF9xPyzYMVNX1wOypigc9hu6ogmYlOYK1QeDcocXn070pHjtQfwtgD9rrt8/S/Yd6UH/bBfg23X+0\nXYAfYl9BdyTBfYfKfh+4oP+376m1Qjeha9Aa1u5tsq/WN0qffAf47VCdu9G9L5vptyQ3Bz5F97n1\nyKpaPVRl4f006RmTN3G25X50MyefTzer8gi6Gah3n3TbJtgnRwFX0E1a2m7gtuVAndcAvwKe3L+p\n/gG4aLBOqzdgBni/fbVOnzwUuJ7usLmdgKf1ffJS+2m9vjoauJDuEMzt+/5YDby75b6iO6ppl/52\nNfDG/t93H7VPgL/p+/ZRdMMvpwLfpT953kq4bayf6ObIHd/3y65Dn+9b3NR+mvjGj6HzXkqXLK8D\nvkU3/jvxdk2wP9bQDZ+sGbr91VC9Q+l2/17bv1l2nnTbp+HGwKGF9tU6fbAv8L2+D84GDpyjjv3U\nfZi/p/9Mugb4Md1x4bdoua+AvQc+iwY/n/5u1D4BbgEcSTd8dzXwOeCuk962peon4J4b+XwfPFRz\nQf3k6YglSWrcsp0zIEmSxsMwIElS4wwDkiQ1zjAgSVLjDAOSJDXOMCBJUuMMA5IkNc4wII0oyZok\nH510OxYiya2THJnkv5PckOT8SbdpoZLMLOf2b8pc25fkmCTDpzmWxsYwoIlKsnf/JbsmyQs2UGdN\nks8vdds2YLmepetg4EC66y48Bzhoss25yZbr6zCq4e2rOcqksTEMaJqs6i9QMhc/CG+axwDfr6qD\nq+oTVXXCpBukeXkhcKtJN0Irl2FA0+LbwF2A/zPphkyDJJslGeeH/3bALzdZa8IWYbunTpKt5vuY\nqrqhuiu1SovCMKBp8Wm6y28enOT2m6q8ofH7JM/tl+05ULaqL7tfksOSXJLkqiSnJLlfX+epSb6b\n5Jok5yd54Uae+9FJvp7k6iSXJjk8yZZz1Ns6yTuT/CjJdUlWJzkuyQ4baPOjkrwxyY/pLtay3yb6\nYPMkByc5K8m1SX6W5J+TPGB43XRX0NtrYEjm0I2s9/wkM0Nlr+sfd/xQ+Tv78jsNlN0xyVFJLkzy\nm36ewgeGX9dNbXeSbZJ8uN+uq5KcmuQhG2jz7km+2L8e1ya5KMmJSf5wY33YP3am3+Ydknwuya+S\nXNH35Q5z1E+Slyb5Tv8euLJ/L+09VG/72b5Osn9f/xrg/Ztq0xzPud6cgdmyJLdN8sEkl/fb/pUk\nD1tou9WmzSfdAKm3BngtcDLwBuBVIzxmvkMHHwOuBN4K/F7/HCf1X4xvobv05y+AFwAfSnJWVf3H\n0DoeQncJ36OBY4B9gFcAD0jymOqv/JVka7rrh98d+L/AmXR7Pl4GfCPJQ6vqv4fW/R66/5MforsU\n99mb2J5P9G05ie7S1XcGDgC+luSPq+p7wL8BzwbeB/y033aA729kvV8GnpVki6q6ri97FN1rtGeS\nm1XV7BfTPsCZVfXToe2+V7/d3wUeTHd10X2SPKyqrtrUdqe7bvu/0l0++ePA1+kux3oy8PPBBye5\nT19+CXA4cDndnpA/Av4A+MZGthW699GWdJev/jrd+/D36V6rhyfZtaouH6j/98DTgX/st3EL4JnA\nyUmeUlXD81ueBNyD7v31N/02LsSG3u//SneZ5DcBdwReCZyYZIehvp5vu9WSSV+y0VvbN9ZesvOV\n/f1/pft1eI+BOmuAE4Yet87lTwfKn9sv23OgbFVf9rmhui/vy3/NwCU+6T5QrwWOm+M51wBPGCo/\nvC/ff6DsCLrLhz5wqO49gCuAj87R5h8wcF3yTfTbY/rHfHKo/A+A3wKnDZVfAJwy4rqf0a/70f39\nW/bb8vG+fLe+fOv+uY4YeOxb+zovGVrny/ryN4+y3cCL+mWHDpUf1JefN1D2ir7soQt8D870jz9s\nqPxJffkHB8qe3Jc9f6juZnSXUB9s1/Z93d8A95lne84bKjsGWDNXGfCBofI/7ctftJB2e2vz5jCB\nps3BdNfj/utFWPeRQ/e/0v89vqouni2sqp8B5wA7zbGOs2v9yXfv6P8+GbrdsXS/uE4DLul3m98x\nyR3prnH/DeCxc6z7g7X2l/imPLn/+9bBwqr6PvB5YI/++Rbi1P7vPv3fR9BNXnsXXZB5VF++F92X\nySlD7VpNt+dk0Ifo9kw8mfXNtd1PAm4A3jtcl27vzqBfzT4myS3nWP8oirWvY1dQdTxwbt+WWc/q\nn/+Eodd1G+ALwPZJ7j207hOr6pwFtmsU7xu6P/v6Db5/R2n3XO93NcIwoKlS3a7tTwLPTPLAMa/+\nvKH7sxPq5jpm/VfAHeYo/8FwQVVdRvclOTu+fCfg9sD/oPsCXD10ezTdMMWwczfe/HXsANw4V3uA\ns/q/289jfb9TVZfSDVHMhoF9gEur6r/ohh0Gy9fQ/ZIdbNc5tXYYYXadNwI/ZG0fDZpru3fsn3Od\nIYXqJtENv47/AHwJeD3wiyRfTvKaJPfY2HYO+VVVrZ6j/AfAtlk7qfF+wG3ohiKGX9dD6ULF8Gs7\nn9d1IdZk9ioqAAAEF0lEQVTpj6qaHUYZfP+O0u5tF7mdmmLOGdA0OoRuV+c7gX3n+diNvadvnGd5\n5vncw487mW4bRnXNAp9vMZwKvCjJbem+9E8dKH9bklv05adX1RU38blu0nb3AeGxSXajC2B7Am+m\nO1T1Gf0v/Jv0FAP/Dl3A+7ON1D9z6P6ivq5VtaG5BBn693zbrYYYBjR1quqCJB8EDkqy1waq/YLu\n1/ewHRevZUD3C2sdSe5MN34++wvtp3R7FrauqlOG64/JeXS76HcG/nNo2c50X2A35Sx9X6ab9Pd4\nYDe6CWez5bcCngDcn27y33C77ptks35vANAd+UA3KW/4V/2GnAc8Jsltqup3wwL9MMCODE0iBKiq\nb9GNf5PkbsDpdBNDRwkD2yTZttadKAjd6726qq7t7/+QLqB+o6quHnFbpsFybbeWiMMEmlZvoZvY\n964NLD8X2H1g9y1JtgGex+KeoOg+SZ44VHZw//d4gH4X+SeAhyV56lwrSTLXMMF8fLb/+7qh9T6A\n7ov6KwO7ixfiVLp+PAS4Of28gH6oYDXdpMyw7nyB2Xbdie6IjEEvpJuY+VlGczxd2Bk+quSldLu7\nfyfJXMM5FwM/oxsTH9Vrh9b7ZLoAMxgmPkb3ufn2uVaQZDF3tc/1vh71vT7JdmsZcM+AplJV/TzJ\nu9nwRMIPAMcCpyQ5Frgd3RfQBYxv7HOuYYL/Ao5N8mHgR8AjgacCM1X1qYF6b6A7tO3TST5NN2nw\neuCedL/Qvk0XXBakqr7Ur/fpfQg6ke5wugPodku/YqHr7tf/yyRnALsA51fVTwYWnwrsT7c9/z70\n0HfRHe54VJIHA9+jOyTwz+nmIWwo3A37KN0RBX/VH+s/e2jhnwI/Zt3PrjcmeQzdRLgL6F63/wXc\nh9GHaX4GPCXJXejmRdyb7giIy+iCDwBV9U/pzm9xYL99J/aPvRvdRMt79bebaq733qhl61nCdmuZ\nMgxomh1G94G83fCCqjqu/+A+kG7G+Y/pjrMuYPiEKws5r/uGHvMd4C+AtwEvoZs4+H66yWuD7ft1\nkj+i+2W7H/BEutnxF9IdxfCROZ5vvp5Jdxz/c+l2119F90X9xqoaHv9dyPpPAR7E+r/+v0wXBr5V\nVeuMhw9s95vo9lA8j+4L9YN0hwkO76Kes11V9dv+C/7ddLP5nwp8k27y5XvpQtWsz9K9R/ajC4LX\n0u05ekFV/d2I23oV3RyIw1n76/n/Aa8aHjqoqucnOZUurLyW7uiXS+lei3X2LizQXO+9Ucs2vNLF\nb7eWsWx47okkrXzpzrZ4j6pa7Pkm0tRyzoAkSY0zDEjSwg8jlVYEw4Ck1i1kTom0ojhnQJKkxrln\nQJKkxhkGJElqnGFAkqTGGQYkSWqcYUCSpMYZBiRJatz/BzN9UHZN6QuKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d268110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(left[:-1], count, width=10)\n",
    "plt.xlabel('Number of words per line')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 55, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e0d499455fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_chars_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_chars_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 55, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/jhalverson/software/spark-1.6.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n    yield next(iterator)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "num_chars_sq = lines.flatMap(lambda line: len(line)**2)\n",
    "for item in num_chars_sq.take(4):\n",
    "    if (item > 100): print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Friday, February 16, 2018\n",
    "# Spark 2 basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NoApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "Row(value=u'# Apache Spark')\n"
     ]
    }
   ],
   "source": [
    "lines = spark.read.text('text_file.md')\n",
    "print lines.count()\n",
    "print lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(value=u'# Apache Spark'), Row(value=u''), Row(value=u'Spark is a fast and general cluster computing system for Big Data. It provides'), Row(value=u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), Row(value=u'supports general computation graphs for data analysis. It also supports a')]\n"
     ]
    }
   ],
   "source": [
    "print lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(value=u'# Apache Spark'), Row(value=u''), Row(value=u'Spark is a fast and general cluster computing system for Big Data. It provides'), Row(value=u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), Row(value=u'supports general computation graphs for data analysis. It also supports a'), Row(value=u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), Row(value=u'MLlib for machine learning, GraphX for graph processing,'), Row(value=u'and Spark Streaming for stream processing.'), Row(value=u''), Row(value=u'<http://spark.apache.org/>'), Row(value=u''), Row(value=u''), Row(value=u'## Online Documentation'), Row(value=u''), Row(value=u'You can find the latest Spark documentation, including a programming'), Row(value=u'guide, on the [project web page](http://spark.apache.org/documentation.html)'), Row(value=u'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).'), Row(value=u'This README file only contains basic setup instructions.'), Row(value=u''), Row(value=u'## Building Spark'), Row(value=u''), Row(value=u'Spark is built using [Apache Maven](http://maven.apache.org/).'), Row(value=u'To build Spark and its example programs, run:'), Row(value=u''), Row(value=u'    build/mvn -DskipTests clean package'), Row(value=u''), Row(value=u'(You do not need to do this if you downloaded a pre-built package.)'), Row(value=u'More detailed documentation is available from the project site, at'), Row(value=u'[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'), Row(value=u''), Row(value=u'## Interactive Scala Shell'), Row(value=u''), Row(value=u'The easiest way to start using Spark is through the Scala shell:'), Row(value=u''), Row(value=u'    ./bin/spark-shell'), Row(value=u''), Row(value=u'Try the following command, which should return 1000:'), Row(value=u''), Row(value=u'    scala> sc.parallelize(1 to 1000).count()'), Row(value=u''), Row(value=u'## Interactive Python Shell'), Row(value=u''), Row(value=u'Alternatively, if you prefer Python, you can use the Python shell:'), Row(value=u''), Row(value=u'    ./bin/pyspark'), Row(value=u''), Row(value=u'And run the following command, which should also return 1000:'), Row(value=u''), Row(value=u'    >>> sc.parallelize(range(1000)).count()'), Row(value=u''), Row(value=u'## Example Programs'), Row(value=u''), Row(value=u'Spark also comes with several sample programs in the `examples` directory.'), Row(value=u'To run one of them, use `./bin/run-example <class> [params]`. For example:'), Row(value=u''), Row(value=u'    ./bin/run-example SparkPi'), Row(value=u''), Row(value=u'will run the Pi example locally.'), Row(value=u''), Row(value=u'You can set the MASTER environment variable when running examples to submit'), Row(value=u'examples to a cluster. This can be a mesos:// or spark:// URL,'), Row(value=u'\"yarn\" to run on YARN, and \"local\" to run'), Row(value=u'locally with one thread, or \"local[N]\" to run locally with N threads. You'), Row(value=u'can also use an abbreviated class name if the class is in the `examples`'), Row(value=u'package. For instance:'), Row(value=u''), Row(value=u'    MASTER=spark://host:7077 ./bin/run-example SparkPi'), Row(value=u''), Row(value=u'Many of the example programs print usage help if no params are given.'), Row(value=u''), Row(value=u'## Running Tests'), Row(value=u''), Row(value=u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests'), Row(value=u'can be run using:'), Row(value=u''), Row(value=u'    ./dev/run-tests'), Row(value=u''), Row(value=u'Please see the guidance on how to'), Row(value=u'[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).'), Row(value=u''), Row(value=u'## A Note About Hadoop Versions'), Row(value=u''), Row(value=u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'), Row(value=u'storage systems. Because the protocols have changed in different versions of'), Row(value=u'Hadoop, you must build Spark against the same version that your cluster runs.'), Row(value=u''), Row(value=u'Please refer to the build documentation at'), Row(value=u'[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'), Row(value=u'for detailed guidance on building for a particular distribution of Hadoop, including'), Row(value=u'building for particular Hive and Hive Thriftserver distributions.'), Row(value=u''), Row(value=u'## Configuration'), Row(value=u''), Row(value=u'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)'), Row(value=u'in the online documentation for an overview on how to configure Spark.')]\n"
     ]
    }
   ],
   "source": [
    "print lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=u''),\n",
       " Row(value=u'## Interactive Python Shell'),\n",
       " Row(value=u'    ./bin/pyspark'),\n",
       " Row(value=u''),\n",
       " Row(value=u'Spark also comes with several sample programs in the `examples` directory.'),\n",
       " Row(value=u''),\n",
       " Row(value=u'package. For instance:'),\n",
       " Row(value=u'[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'),\n",
       " Row(value=u'')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(withReplacement=False, fraction=0.1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "plines = lines.rdd.filter(lambda row: 'Python' in row.value or 'Spark' in row.value)\n",
    "print plines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               value|Length|\n",
      "+--------------------+------+\n",
      "|      # Apache Spark|    14|\n",
      "|                    |     0|\n",
      "|Spark is a fast a...|    78|\n",
      "|high-level APIs i...|    75|\n",
      "|supports general ...|    73|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chars = lines.withColumn('Length', F.length(lines.value))\n",
    "chars.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 78, 75, 73, 74, 56, 42, 0, 26]\n"
     ]
    }
   ],
   "source": [
    "chars = lines.rdd.map(lambda row: len(row.value))\n",
    "print chars.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'fish', 'cat', 'mouse', Row(value=u'# Apache Spark'), Row(value=u'Spark is a fast and general cluster computing system for Big Data. It provides'), Row(value=u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), Row(value=u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), Row(value=u'and Spark Streaming for stream processing.'), Row(value=u'You can find the latest Spark documentation, including a programming'), Row(value=u'## Building Spark'), Row(value=u'Spark is built using [Apache Maven](http://maven.apache.org/).'), Row(value=u'To build Spark and its example programs, run:'), Row(value=u'[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'), Row(value=u'The easiest way to start using Spark is through the Scala shell:'), Row(value=u'## Interactive Python Shell'), Row(value=u'Alternatively, if you prefer Python, you can use the Python shell:'), Row(value=u'Spark also comes with several sample programs in the `examples` directory.'), Row(value=u'    ./bin/run-example SparkPi'), Row(value=u'    MASTER=spark://host:7077 ./bin/run-example SparkPi'), Row(value=u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests'), Row(value=u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'), Row(value=u'Hadoop, you must build Spark against the same version that your cluster runs.'), Row(value=u'in the online documentation for an overview on how to configure Spark.')]\n"
     ]
    }
   ],
   "source": [
    "small = spark.sparkContext.parallelize(['dog', 'fish', 'cat', 'mouse'])\n",
    "small_and_keys = small.union(plines)\n",
    "print small_and_keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'fish', 'cat', 'mouse', 14, 0, 78, 75, 73, 74, 56, 42, 0, 26, 0, 0, 23, 0, 68, 76]\n"
     ]
    }
   ],
   "source": [
    "small_and_ints = small.union(chars)\n",
    "print small_and_ints.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 42\n"
     ]
    }
   ],
   "source": [
    "print chars.count(), chars.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120\n"
     ]
    }
   ],
   "source": [
    "# find the maximum\n",
    "print chars.reduce(lambda x, y: x if x > y else y), chars.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 78, 75, 73, 74, 56, 42, 0, 26, 0, 0, 23, 0, 68, 76, 70, 56, 0, 17, 0, 62, 45, 0, 39, 0, 67, 66, 76, 0, 26, 0, 64, 0, 21, 0, 52, 0, 44, 0, 27, 0, 66, 0, 17, 0, 61, 0, 43, 0, 19, 0, 74, 74, 0, 29, 0, 32, 0, 75, 62, 41, 73, 72, 22, 0, 54, 0, 69, 0, 16, 0, 84, 17, 0, 19, 0, 33, 120, 0, 31, 0, 77, 76, 77, 0, 42, 120, 84, 65, 0, 16, 0, 97, 70]\n"
     ]
    }
   ],
   "source": [
    "print chars.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'#', 1), (u'Apache', 1), (u'Spark', 1), (u'Spark', 1), (u'is', 1)]\n"
     ]
    }
   ],
   "source": [
    "pairs = lines.rdd.flatMap(lambda row: row.value.split()).map(lambda x: (x, 1))\n",
    "print pairs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'cat', 'dog', 'dog', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# note that we change types here from int to string\n",
    "trans = chars.map(lambda x: 'dog' if x > 10 else 'cat')\n",
    "print trans.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {0: 35, 14: 1, 16: 2, 17: 3, 19: 2, 21: 1, 22: 1, 23: 1, 26: 2, 27: 1, 29: 1, 31: 1, 32: 1, 33: 1, 39: 1, 41: 1, 42: 2, 43: 1, 44: 1, 45: 1, 52: 1, 54: 1, 56: 2, 61: 1, 62: 2, 64: 1, 65: 1, 66: 2, 67: 1, 68: 1, 69: 1, 70: 2, 72: 1, 73: 2, 74: 3, 75: 2, 76: 3, 77: 2, 78: 1, 84: 2, 97: 1, 120: 2})\n"
     ]
    }
   ],
   "source": [
    "print chars.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 120, 97, 84, 84]\n"
     ]
    }
   ],
   "source": [
    "print chars.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[31] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that persist does not force evaluation\n",
    "chars.persist(StorageLevel(True, True, False, False, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

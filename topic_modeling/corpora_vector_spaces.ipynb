{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Tuesday, February 7, 2017\n",
    "# Gensim: Corpora and vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import logging\n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 19:15:30,620 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    ">>> from gensim import corpora\n",
    ">>>\n",
    ">>> documents = [\"Human machine interface for lab abc computer applications\",\n",
    ">>>              \"A survey of user opinion of computer system response time\",\n",
    ">>>              \"The EPS user interface management system\",\n",
    ">>>              \"System and human system engineering testing of EPS\",\n",
    ">>>              \"Relation of user perceived response time to error measurement\",\n",
    ">>>              \"The generation of random binary unordered trees\",\n",
    ">>>              \"The intersection graph of paths in trees\",\n",
    ">>>              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    ">>>              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> # remove common words and tokenize\n",
    ">>> stoplist = set('for a of the and to in'.split())\n",
    ">>> texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> # remove words that appear only once in the corpus\n",
    ">>> from collections import defaultdict\n",
    ">>> frequency = defaultdict(int)\n",
    ">>> for text in texts:\n",
    ">>>     for token in text:\n",
    ">>>         frequency[token] += 1\n",
    "\n",
    ">>> texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    ">>> from pprint import pprint  # pretty-printer\n",
    ">>> pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative method to remove words that appear only once in the corpus\n",
    "from collections import Counter\n",
    "c = Counter(reduce(lambda u, v: u + v, texts))\n",
    "texts = [[token for token in text if c[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    ">>> from pprint import pprint  # pretty-printer\n",
    ">>> pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 19:15:30,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-08 19:15:30,675 : INFO : built Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...) from 9 documents (total 29 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    ">>> dictionary = corpora.Dictionary(texts)\n",
    ">>> print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    ">>> print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    ">>> new_doc = \"Human computer interaction\"\n",
    ">>> new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    ">>> print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(0, 1), (6, 1), (7, 1), (8, 1)],\n",
      " [(2, 1), (6, 2), (8, 1)],\n",
      " [(3, 1), (4, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(5, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    ">>> corpus = [dictionary.doc2bow(text, return_missing=False) for text in texts]\n",
    ">>> pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can write a class which overrides the \"__iter__\" function to generate one document at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways to serialize a corpus using different formats. The reading and writing is done in a streaming fashing meaning one document at a time. This makes it possible to work with large corpora within getting into problems with memory limitations. A dictionary of the vocabulary can also be constructed without loads all the documents (see the six module). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show how to transform documents from one vector representation into another. This process serves two goals:\n",
    "\n",
    "     \n",
    "-- To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.\n",
    "\n",
    "-- To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy (marginal data trends are ignored, noise-reduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, u'minors'),\n",
       " (10, u'graph'),\n",
       " (6, u'system'),\n",
       " (9, u'trees'),\n",
       " (8, u'eps'),\n",
       " (1, u'computer'),\n",
       " (5, u'survey'),\n",
       " (7, u'user'),\n",
       " (2, u'human'),\n",
       " (4, u'time'),\n",
       " (0, u'interface'),\n",
       " (3, u'response')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(0, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (6, 2), (8, 1)],\n",
       " [(3, 1), (4, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(5, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 19:15:30,746 : INFO : collecting document frequencies\n",
      "2017-02-08 19:15:30,747 : INFO : PROGRESS: processing document #0\n",
      "2017-02-08 19:15:30,748 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    ">>> tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, tfidf is treated as a read-only object that can be used to convert any vector from the old representation (bag-of-words integer counts) to the new representation (TfIdf real-valued weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    ">>> doc_bow = [(0, 1), (1, 1)]\n",
    ">>> print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(1, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.44424552527467476), (6, 0.3244870206138555), (7, 0.3244870206138555)]\n",
      "[(0, 0.5710059809418182), (6, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(2, 0.49182558987264147), (6, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (4, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(5, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    ">>> corpus_tfidf = tfidf[corpus]\n",
    ">>> for doc in corpus_tfidf:\n",
    "...     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 19:15:30,770 : INFO : using serial LSI version on this node\n",
      "2017-02-08 19:15:30,771 : INFO : updating model with new documents\n",
      "2017-02-08 19:15:30,772 : INFO : preparing a new chunk of documents\n",
      "2017-02-08 19:15:30,773 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-02-08 19:15:30,774 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2017-02-08 19:15:30,775 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2017-02-08 19:15:30,777 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2017-02-08 19:15:30,780 : INFO : computing the final decomposition\n",
      "2017-02-08 19:15:30,781 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2017-02-08 19:15:30,782 : INFO : processed documents up to #9\n",
      "2017-02-08 19:15:30,784 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2017-02-08 19:15:30,785 : INFO : topic #1(1.476): 0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"\n"
     ]
    }
   ],
   "source": [
    ">>> lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    ">>> corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set num_topics=2). Now you’re probably wondering: what do these two latent dimensions stand for? Let’s inspect with models.LsiModel.print_topics():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 19:15:30,792 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2017-02-08 19:15:30,794 : INFO : topic #1(1.476): 0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  u'0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that according to LSI, “trees”, “graph” and “minors” are all related words (and contribute the most to the direction of the first topic), while the second topic practically concerns itself with all the other words. As expected, the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066007833960903012), (1, 0.52007033063618546)]\n",
      "[(0, 0.19667592859142469), (1, 0.76095631677000541)]\n",
      "[(0, 0.089926399724462758), (1, 0.72418606267525143)]\n",
      "[(0, 0.075858476521780169), (1, 0.63205515860034289)]\n",
      "[(0, 0.10150299184980158), (1, 0.57373084830029608)]\n",
      "[(0, 0.70321089393783098), (1, -0.16115180214025687)]\n",
      "[(0, 0.87747876731198304), (1, -0.16758906864659276)]\n",
      "[(0, 0.90986246868185772), (1, -0.14086553628718873)]\n",
      "[(0, 0.61658253505692795), (1, 0.053929075663894696)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, -0.617), (1, 0.054)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "...     print(doc)\n",
    "[(0, -0.066), (1, 0.520)] # \"Human machine interface for lab abc computer applications\"\n",
    "[(0, -0.197), (1, 0.761)] # \"A survey of user opinion of computer system response time\"\n",
    "[(0, -0.090), (1, 0.724)] # \"The EPS user interface management system\"\n",
    "[(0, -0.076), (1, 0.632)] # \"System and human system engineering testing of EPS\"\n",
    "[(0, -0.102), (1, 0.574)] # \"Relation of user perceived response time to error measurement\"\n",
    "[(0, -0.703), (1, -0.161)] # \"The generation of random binary unordered trees\"\n",
    "[(0, -0.877), (1, -0.168)] # \"The intersection graph of paths in trees\"\n",
    "[(0, -0.910), (1, -0.141)] # \"Graph minors IV Widths of trees and well quasi ordering\"\n",
    "[(0, -0.617), (1, 0.054)] # \"Graph minors A survey\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

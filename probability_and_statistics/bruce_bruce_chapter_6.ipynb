{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Monday, August 7, 2017\n",
    "# Chapter 6: Statistical machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure-based for data-driven methods: Linear and logistic regression are said to be structure-based while KNN and tree methods are data-driven. Tree methods were released in 1984 by Breiman and Friedman in California. Bagging and boosting came out in the 1990's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is suggested to use Mahalanobis distance when working with features that may be correlated. This will treat correlate features as one instead of as individuals which will weight the distance. Gower's distance may also be considered since it handles mixed continuous and indicator variables and prevents the indicator variables from dominating the calculation of distance or variance. Mahalanobis is expensive and requires the calculation of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[-0.1, 1, 2.8],\n",
    "     [3, 4.2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.9689033],\n",
       "       [ 4.9689033,  0.       ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a convenience routine for the sake of testing.  For many\n",
    "metrics, the utilities in scipy.spatial.distance.cdist and\n",
    "scipy.spatial.distance.pdist will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  8.5],\n",
       "       [ 8.5,  0. ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = DistanceMetric.get_metric('manhattan')\n",
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dist = DistanceMetric.get_metric('mahalanobis', V=np.cov(X))\n",
    "#dist.pairwise(X) appears that this has not been implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is very important to standardize the features when using KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear and logistic regression, one hot encoding causes a multicollinearity error. It works fine for tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-varance tradeoff: "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

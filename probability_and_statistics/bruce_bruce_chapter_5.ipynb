{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Thursday, August 3, 2017\n",
    "# Chapter 5 of Bruce and Bruce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to predict the nature of Wikipedia biographies using only three records per class to try the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_and_tokenize(person):\n",
    "     # download and parse the biography\n",
    "     base_url = 'https://en.wikipedia.org/wiki/'\n",
    "     r = requests.get(base_url + person)\n",
    "     soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "     # extract the text of each paragraph\n",
    "     raw_text = ''\n",
    "     for paragraph in soup.find_all('p'):\n",
    "          raw_text += paragraph.get_text()\n",
    "    \n",
    "     # keep only alphabetical characters and split on whitespace\n",
    "     letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "     words = letters_only.lower().split()\n",
    "\n",
    "     # count the words and filter based on count and stopwords, apply stemming\n",
    "     count = Counter(words)\n",
    "     porter = PorterStemmer()\n",
    "     stops = stopwords.words(\"english\")\n",
    "     words = [porter.stem(word) for word in words if (word not in stops) and (count[word] > 1) and (len(word) > 1)]\n",
    "     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "einstein = scrape_and_tokenize('Albert_Einstein')\n",
    "newton = scrape_and_tokenize('Isaac_Newton')\n",
    "darwin = scrape_and_tokenize('Charles_Darwin')\n",
    "spielberg = scrape_and_tokenize('Steven_Spielberg')\n",
    "allen = scrape_and_tokenize('Woody_Allen')\n",
    "cameron = scrape_and_tokenize('James_Cameron')\n",
    "jordan = scrape_and_tokenize('Michael_Jordan')\n",
    "brady = scrape_and_tokenize('Tom_Brady')\n",
    "williams = scrape_and_tokenize('Serena_Williams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'albert',\n",
       " u'einstein',\n",
       " u'german',\n",
       " u'march',\n",
       " u'april',\n",
       " u'german',\n",
       " u'born',\n",
       " u'theoret',\n",
       " u'physicist',\n",
       " u'einstein']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einstein[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Naive Bayes is to calculate $P(Y|X_j)$ based on $P(X_j|Y)$. That is, knowin the probability of all the features being associated with a given class, given the features of a new record what is the most likely class? In this example, we have three records for each type of person. For each class we will compute the probably of each word being found. Then when a new biography is presented we will figure out which class it falls into based on the words in that record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Y_i|X_1, X_2, ..., X_N)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(einstein + newton + darwin + spielberg + allen + cameron + jordan + brady + williams)\n",
    "c_scientist = Counter(einstein + newton + darwin)\n",
    "c_filmmaker = Counter(spielberg + allen + cameron)\n",
    "c_athlete = Counter(jordan + brady + williams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0.5\n",
    "p_scientist = {}\n",
    "for word, count in c_scientist.items():\n",
    "     p_scientist[word] = (count + k) / float(c[word] + 2 * k)\n",
    "p_filmmaker = {}\n",
    "for word, count in c_filmmaker.items():\n",
    "     p_filmmaker[word] = (count + k) / float(c[word] + 2 * k)\n",
    "p_athlete = {}\n",
    "for word, count in c_athlete.items():\n",
    "     p_athlete[word] = (count + k) / float(c[word] + 2 * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'four', 0.13218390804597702),\n",
       " (u'captain', 0.8333333333333334),\n",
       " (u'whose', 0.6428571428571429),\n",
       " (u'deviat', 0.8333333333333334),\n",
       " (u'hermann', 0.875),\n",
       " (u'everi', 0.40476190476190477),\n",
       " (u'rise', 0.9166666666666666),\n",
       " (u'quantiz', 0.9),\n",
       " (u'govern', 0.6428571428571429),\n",
       " (u'disturb', 0.8333333333333334)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scientist.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider new records we must keep in mind that there will be words that did not appear in the training set. These words must be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kubrick = scrape_and_tokenize('Stanley_Kubrick')\n",
    "kubrick = [word for word in kubrick if word in c.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "reduce(operator.mul, np.log(p_scientist.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a continuous feature, Gaussian Naive Bayes fit the feature to a Gaussian for each class and uses the pdf to generate the required conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the weights that maximize class separability. The idea is to maximize the ratio of distance between the class centriods over the variance within each class, weighted by the covariance matrix. Bruce & Bruce treat it as a model whereas other sources treat it as a dimensionality reduction technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

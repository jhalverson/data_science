{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Thursday, August 3, 2017\n",
    "# Chapter 5 of Bruce and Bruce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to predict the nature of Wikipedia biographies using only three records per class to try the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_and_tokenize(person):\n",
    "     # download and parse the biography\n",
    "     base_url = 'https://en.wikipedia.org/wiki/'\n",
    "     r = requests.get(base_url + person)\n",
    "     soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "     # extract the text of each paragraph\n",
    "     raw_text = ''\n",
    "     for paragraph in soup.find_all('p'):\n",
    "          raw_text += paragraph.get_text()\n",
    "    \n",
    "     # keep only alphabetical characters and split on whitespace\n",
    "     letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "     words = letters_only.lower().split()\n",
    "\n",
    "     # count the words and filter based on count and stopwords, apply stemming\n",
    "     count = Counter(words)\n",
    "     porter = PorterStemmer()\n",
    "     stops = stopwords.words(\"english\")\n",
    "     words = [porter.stem(word) for word in words if (word not in stops) and (count[word] > 1) and (len(word) > 1)]\n",
    "     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "einstein = scrape_and_tokenize('Albert_Einstein')\n",
    "newton = scrape_and_tokenize('Isaac_Newton')\n",
    "darwin = scrape_and_tokenize('Charles_Darwin')\n",
    "spielberg = scrape_and_tokenize('Steven_Spielberg')\n",
    "allen = scrape_and_tokenize('Woody_Allen')\n",
    "cameron = scrape_and_tokenize('James_Cameron')\n",
    "jordan = scrape_and_tokenize('Michael_Jordan')\n",
    "brady = scrape_and_tokenize('Tom_Brady')\n",
    "williams = scrape_and_tokenize('Serena_Williams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'albert',\n",
       " u'einstein',\n",
       " u'german',\n",
       " u'march',\n",
       " u'april',\n",
       " u'german',\n",
       " u'born',\n",
       " u'theoret',\n",
       " u'physicist',\n",
       " u'einstein']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einstein[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Naive Bayes is to calculate $P(Y|X_j)$ based on $P(X_j|Y)$. That is, knowin the probability of all the features being associated with a given class, given the features of a new record what is the most likely class? In this example, we have three records for each type of person. For each class we will compute the probably of each word being found. Then when a new biography is presented we will figure out which class it falls into based on the words in that record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is done below computes the probability based on the number times a word appears per record. The correct way of doing this is to work with the probability of a word appearing in a biography (i.e., one asks does the word 'data' appear in the biography or does it not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(einstein + newton + darwin + spielberg + allen + cameron + jordan + brady + williams)\n",
    "c_scientist = Counter(einstein + newton + darwin)\n",
    "c_filmmaker = Counter(spielberg + allen + cameron)\n",
    "c_athlete = Counter(jordan + brady + williams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0.5\n",
    "p_scientist = {}\n",
    "for word, count in c_scientist.items():\n",
    "     p_scientist[word] = (count + k) / float(c[word] + 2 * k)\n",
    "p_filmmaker = {}\n",
    "for word, count in c_filmmaker.items():\n",
    "     p_filmmaker[word] = (count + k) / float(c[word] + 2 * k)\n",
    "p_athlete = {}\n",
    "for word, count in c_athlete.items():\n",
    "     p_athlete[word] = (count + k) / float(c[word] + 2 * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'four', 0.13218390804597702),\n",
       " (u'captain', 0.8333333333333334),\n",
       " (u'whose', 0.6428571428571429),\n",
       " (u'deviat', 0.8333333333333334),\n",
       " (u'hermann', 0.875),\n",
       " (u'everi', 0.40476190476190477),\n",
       " (u'rise', 0.9166666666666666),\n",
       " (u'quantiz', 0.9),\n",
       " (u'govern', 0.6428571428571429),\n",
       " (u'disturb', 0.8333333333333334)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scientist.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider new records we must keep in mind that there will be words that did not appear in the training set. These words must be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kubrick = scrape_and_tokenize('Stanley_Kubrick')\n",
    "kubrick = [word for word in kubrick if word in c.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-811.44216321963813"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.log(p_scientist.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a continuous feature, Gaussian Naive Bayes fit the feature to a Gaussian for each class and uses the pdf to generate the required conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the weights that maximize class separability. The idea is to maximize the ratio of distance between the class centriods over the variance within each class, weighted by the covariance matrix. Bruce & Bruce treat it as a model whereas other sources treat it as a dimensionality reduction technique. There is also quadratic discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is used to find a linear combination of features that separates two or more classes. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach starts by assuming that the features are normally distributed. This gives QDA. The further assumption of the covariance of each class being is equal leads to LDA. The multivariate normal is specified by the mean of each feature and covariance matrix. Note that it does not assume that each feature within each class is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on detecting multicollinearity: https://onlinecourses.science.psu.edu/stat501/node/347 See VIF or variance inflation factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever doing a classification problem one should always count the outcomes and see if they are balanced. Imagine the cause of fraud detection where there are 1 million zeroes and 100 ones. If all the data are used to fit the model, it is possible that some of the zeros cases will have very similar feature to the ones cases just by chance (since there are so many zero cases). This will make it unlikely for a model to be found which can distinguish between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a vast number of records for each class, one could undersample by throwing away the records of the class with the most records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach throws away data which may not be ideal. Another approach is to weight the minority records so that the weights of the two are the same. So if there are 1000 zeroes and 25 ones, then weight the zeros with w=1 and the ones with weight w=40. The software must support this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it will always be interesting to compare the result of using an imbalanced class technique with using all the data. If you the result is the same it suggests that one of the class is quite difference from the other in terms of the features since chance does not cause the two to be indistiguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to increase the number of minority records by using bootstrapping. That is, draw a bootstrapped resample of the same size as the majority records. I don't like this as much as just weighting the minority since more computation is needed with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, there is data generation or methods like SMOTE. Here, new samples from the minority are created. Choose a minority record at random. Use KNN to find a similar minorty record. For each predictor choose a weight between 0 and 1 and create a linear combination with w and 1 - w as the weights between the two records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of loan data, the idea is often to predict the probability of the loan being repaid versus defaulting. So the cost can be written as $C=P(y=0)R + P(y=1)D$, where R is the value gained on repayment and D is the value gained on default (D is less than zero). With estimates of R and D one can go one step further than just recall (sensitivity -- proportion of 1's correctly classified) and specificity to optimize the operating parameters of the model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Jonathan Halverson\n",
    "# Tuesday, August 22, 2017\n",
    "# Chapter 7: Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning may be used for clustering, data visualization, dimensionality reduction and anomaly detection. It can also be used to derive new features. UL may be used to deal with the cold start problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA is a projection method. The idea is to find successive hyperplanes that minimize the sum of the squares between the records and the plane. The data is then projected into the hyperplane. Then is then repeated keeping mind that the planes must be orthogonal to each other. The data should be centered and scaled. This is equivalent to find the eigenvectors and values of the normalized covariance matrix and projecting the data into a new subspace using the eigenvectors as the columns in the transformation matrix. Choose the eigenvectors with the largest corresponding eigenvalues up to some percentage of the total explained variance. Note that in scikit-learn you can enter the number of principal components as a fraction of 1 which is the explained variance.\n",
    "\n",
    "The method is limited in that it is a projection method and it only considers uncorrelated linear combinations of the features. The interpretation of the original features is lost but it should speed up training and in some cases it will remove noise which will lead to a robust, properly fit model. Be careful with a mix of numeric and indicator variables. Consider using cross-validation to decide on how many components to use. One can inspect the loadings for insight into the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is a clustering method. Its advantages are that it scales well and is simple. Must be careful with mixed continuous and indicator variables since K-means can put all the records with 1's in one cluster to minimize the penalty. The method works by initializing K points and assigning records to those points according to shortest distance. The centroid of each cluster is computed along with the sum of the in-cluster error which is the sum of the square per cluster added up. Records and then reassigned until convergence or some tolerance. The elbow method is used to find the optimal number of clusters. Note that K-means imposes a spheriodal structure on the cluster. It will fail for many cases such as the half moon case. In general cluster methods produce different outcomes depending on the method uses. It is important to put forth a hypothesis before applying clustering methods because if you try hard enough you will probably find something but it may not be significant. Note that clusters can be empty in K-means. The method generally runs 10 times or so and report the best result for each value of K. Data should of course be standardized. One can examine the mean values of the records in each cluster for meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is expensive but the internal nature of each cluster can be investigated by looking at a dendrogram. The bottom-up approach is to link clusters by a measure of similarity. Ward's linkage would result in the maximum lowering of the sum of the variance, the maximum distance between records in a cluster (or min) and the centers of the clusters are all possible choices. It is better at identifying outlying or aberrant groups than k-means. This method does not scale well because all pairwise distances are computed in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-based clustering is rooted in statistical theory and these techniques provide a more rigorous way to determine the nature and number of clusters. The idea is that the records are distributed from K-multivariate normal distributions. The BIC is used to to determine the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gower's distance can be used to compute the distance between two records that are composed on continuous numeric and indicator variables. Even when normalized there can be large differences between such variables. Manhattan distance is used between continuous and ordinal data while the distance is 1 if the indicators are different and 0 otherwise. In the end all Gower's distances are between 0 and 1."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

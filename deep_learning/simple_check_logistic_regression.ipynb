{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Thursday, December 14, 2017\n",
    "# Simple check with Tensorflow and class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created because I was having trouble getting tensorflow to do logistic regression on a pair of hand-written digits. There is less preprocessing when using the breast cancer dataset so that was used here. The big takeaway is that when the number of features is large, the coefficients differ markedly between Sklearn, the self-written code and Tensorflow while the accuracy is the same for all. This is somewhat surprising since the optimization space is guaranteed to be convex since the loss function is log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_digits\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = load_breast_cancer().data\n",
    "y = load_breast_cancer().target\n",
    "#X = load_digits().data\n",
    "#y = load_digits().target\n",
    "#X = X[(y == 0) | (y == 1)]\n",
    "#y = y[(y == 0) | (y == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212, 357])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a class imbalance so we will weight the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = y.size / (2.0 * np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tensorflow code below did not converge until the data were standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "from sklearn.decomposition import PCA\n",
    "X_std = PCA(n_components=5).fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA makes the tensorflow model much easier to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.977152899824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "wts = {0:w[0], 1:w[1]}\n",
    "#lr = LogisticRegression(C=1e12, class_weight=wts)\n",
    "#lr = LogisticRegression(C=1e12, class_weight='balanced')\n",
    "lr = LogisticRegression(C=1e12, class_weight=None, tol=1e-7, verbose=10)\n",
    "\n",
    "print lr.fit(X_std, y).score(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.3419811320754718, 1: 0.79691876750700286}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.073283324890973389"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = 1e-7\n",
    "y_pred_prob = lr.fit(X_std, y).predict_proba(X_std)\n",
    "loss = -(np.log(y_pred_prob[y == 0][:,0] + ep).sum() + np.log(y_pred_prob[y == 1][:,1] + ep).sum()) / y_pred_prob.shape[0]\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07328354664840192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(y, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4137167 , -2.89107656,  1.59169636,  0.4984996 ,  0.78415435,\n",
       "        1.28067847])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(lr.intercept_, lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-written version gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "epochs = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.random.randn(X_std.shape[1] + 1)\n",
    "X_bias = np.c_[np.ones((X_std.shape[0], 1)), X_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.13948203498\n",
      "100000 0.0732835466484\n",
      "200000 0.0732835466484\n",
      "300000 0.0732835466484\n",
      "400000 0.0732835466484\n",
      "500000 0.0732835466484\n",
      "600000 0.0732835466484\n",
      "700000 0.0732835466484\n",
      "800000 0.0732835466484\n",
      "900000 0.0732835466484\n",
      "1000000 0.0732835466484\n",
      "[ 0.41371574 -2.89107853  1.59169789  0.49849996  0.78415443  1.28068096]\n",
      "accuracy = 0.977152899824\n"
     ]
    }
   ],
   "source": [
    "for epoch in xrange(epochs + 1):\n",
    "     y_prob = expit(np.dot(X_bias, theta))\n",
    "     errors = y_prob - y\n",
    "     gradients = (1.0 / X_bias.shape[0]) * np.dot(X_bias.T, errors)\n",
    "     theta = theta - learning_rate * gradients\n",
    "     if not (epoch % 100000): print epoch, log_loss(y, y_prob)\n",
    "print(theta)\n",
    "print \"accuracy =\", accuracy_score(y, y_prob > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_weights = y.copy().astype(np.float64)\n",
    "#my_weights[y == 0] = w[0]\n",
    "#my_weights[y == 1] = w[1]\n",
    "my_weights = np.ones(y.size, dtype=np.float64)\n",
    "my_weights = tf.constant(my_weights.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.constant(X_std, dtype=tf.float64)\n",
    "y = tf.constant(y.reshape(-1, 1), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_init = tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=42, dtype=tf.float64)\n",
    "prob_positive = tf.layers.dense(X, units=1, activation=tf.sigmoid, kernel_initializer=k_init, name='single_neuron')\n",
    "loss = tf.losses.log_loss(labels=y, predictions=prob_positive, weights=my_weights, epsilon=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one could code LR manually using gradient descent and compare the code below to that for\n",
    "# each step (see if loss is the same)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.975, use_nesterov=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The accuracy metric creates a local variables. It also stores a running average. Try\n",
    "# evaluating accuracy throughout the minimization and then accuracy2 at the very end (they\n",
    "# will differ).\n",
    "#accuracy, accuracy_op = tf.metrics.accuracy(labels=y, predictions=tf.round(prob_positive))\n",
    "#accuracy2, accuracy_op2 = tf.metrics.accuracy(labels=y, predictions=tf.round(prob_positive))\n",
    "\n",
    "# all three definitions give the same result\n",
    "acc = tf.reduce_sum(tf.cast(tf.equal(y, tf.cast(prob_positive + 0.5, tf.int32)), tf.int32))\n",
    "acc2 = tf.reduce_sum(tf.cast(tf.equal(y, tf.cast(tf.round(prob_positive), tf.int32)), tf.int32))\n",
    "acc3 = tf.reduce_sum(tf.cast(tf.equal(y, tf.cast(prob_positive > 0.5, tf.int32)), tf.int32))\n",
    "\n",
    "tot = tf.cast(tf.size(y), tf.float64)\n",
    "acc = tf.cast(acc, tf.float64) / tot\n",
    "acc2 = tf.cast(acc2, tf.float64) / tot\n",
    "acc3 = tf.cast(acc3, tf.float64) / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#lcl = tf.local_variables_initializer() # accuracy metric creates local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 2.43958 Acc: 0.203866432337 0.203866432337 0.203866432337\n",
      "10000 Loss: 0.0740872 Acc: 0.978910369069 0.978910369069 0.978910369069\n",
      "20000 Loss: 0.0732833 Acc: 0.977152899824 0.977152899824 0.977152899824\n",
      "30000 Loss: 0.0732833 Acc: 0.977152899824 0.977152899824 0.977152899824\n",
      "40000 Loss: 0.0732833 Acc: 0.977152899824 0.977152899824 0.977152899824\n",
      "50000 Loss: 0.0732834 Acc: 0.977152899824 0.977152899824 0.977152899824\n",
      "[[-2.8911071 ]\n",
      " [ 1.59170036]\n",
      " [ 0.4985112 ]\n",
      " [ 0.78414355]\n",
      " [ 1.28070066]]\n",
      "[ 0.41371956]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     #lcl.run()\n",
    "     sess.run(training_op)\n",
    "     for epoch in xrange(n_epochs + 1): \n",
    "          sess.run(training_op)\n",
    "          if not (epoch % 10000): print epoch, \"Loss:\", loss.eval(), \"Acc:\", acc.eval(), acc2.eval(), acc3.eval()\n",
    "     print tf.get_default_graph().get_tensor_by_name('single_neuron/kernel:0').eval()\n",
    "     print tf.get_default_graph().get_tensor_by_name('single_neuron/bias:0').eval()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

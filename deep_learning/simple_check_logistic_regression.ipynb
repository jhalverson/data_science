{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = load_breast_cancer().data\n",
    "y = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tensorflow code below did not converge until the data were standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98769771529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "print lr.fit(X_std, y).score(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35372245, -0.3850941 , -0.34237238, -0.44138446, -0.15523716,\n",
       "         0.5681635 , -0.8685186 , -0.96811443,  0.07328189,  0.31122062,\n",
       "        -1.29527365,  0.26995006, -0.6662383 , -1.02954508, -0.2812678 ,\n",
       "         0.74241788,  0.11352258, -0.32006685,  0.28982672,  0.67152689,\n",
       "        -1.0304876 , -1.3131883 , -0.82563973, -1.02915516, -0.67185301,\n",
       "         0.04896119, -0.87162239, -0.91131563, -0.8839543 , -0.48354624]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17993455])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026659267587480589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = lr.fit(X_std, y).predict_proba(X_std)\n",
    "loss = -(np.log(y_pred_prob[y == 0][:,0]).sum() + np.log(y_pred_prob[y == 1][:,1]).sum()) / y_pred_prob.size\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.constant(X_std)\n",
    "y = tf.constant(y.reshape(-1, 1), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_init = tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=42)\n",
    "prob_positive = tf.layers.dense(X, units=1, activation=tf.sigmoid, kernel_initializer=k_init, name='single_neuron')\n",
    "loss = tf.losses.log_loss(labels=y, predictions=prob_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=1.0, momentum=0.975, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy, accuracy_op = tf.metrics.accuracy(labels=y, predictions=tf.round(prob_positive))\n",
    "accuracy2, accuracy_op2 = tf.metrics.accuracy(labels=y, predictions=tf.round(prob_positive))\n",
    "acc = tf.reduce_sum(tf.cast(tf.equal(y, tf.cast(prob_positive + 0.5, tf.int32)), tf.int32))\n",
    "acc2 = tf.reduce_sum(tf.cast(tf.equal(y, tf.cast(tf.round(prob_positive), tf.int32)), tf.int32))\n",
    "numm = tf.cast(tf.size(y), tf.float32)\n",
    "jj = tf.cast(prob_positive + 0.5, tf.int32)\n",
    "hh = tf.cast(tf.round(prob_positive), tf.int32)\n",
    "vv = tf.cast(prob_positive > 0.5, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "lcl = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50177345016e-19 0.999999999999\n",
      "Loss: 0.225255 Acc: 0.0 0.947276 0.947275922671 0.947275922671\n",
      "Loss: 0.155675 Acc: 0.947276 0.967487 0.98769771529 0.98769771529\n",
      "Loss: 0.14224 Acc: 0.967487 0.975395 0.991212653779 0.991212653779\n",
      "Loss: 0.14183 Acc: 0.975395 0.97935 0.991212653779 0.991212653779\n",
      "Loss: 0.141777 Acc: 0.97935 0.981722 0.991212653779 0.991212653779\n",
      "Loss: 0.141752 Acc: 0.981722 0.983304 0.991212653779 0.991212653779\n",
      "Loss: 0.141736 Acc: 0.983304 0.984434 0.991212653779 0.991212653779\n",
      "Loss: 0.141725 Acc: 0.984434 0.985281 0.991212653779 0.991212653779\n",
      "Loss: 0.141716 Acc: 0.985281 0.98594 0.991212653779 0.991212653779\n",
      "Loss: 0.141708 Acc: 0.98594 0.986467 0.991212653779 0.991212653779\n",
      "Loss: 0.141702 Acc: 0.986467 0.986899 0.991212653779 0.991212653779\n",
      "Loss: 0.141697 Acc: 0.986899 0.987258 0.991212653779 0.991212653779\n",
      "Loss: 0.141693 Acc: 0.987258 0.987563 0.991212653779 0.991212653779\n",
      "Loss: 0.141689 Acc: 0.987563 0.987823 0.991212653779 0.991212653779\n",
      "Loss: 0.141686 Acc: 0.987823 0.988049 0.991212653779 0.991212653779\n",
      "Loss: 0.141683 Acc: 0.988049 0.988247 0.991212653779 0.991212653779\n",
      "Loss: 0.14168 Acc: 0.988247 0.988421 0.991212653779 0.991212653779\n",
      "Loss: 0.141678 Acc: 0.988421 0.988576 0.991212653779 0.991212653779\n",
      "Loss: 0.141676 Acc: 0.988576 0.988715 0.991212653779 0.991212653779\n",
      "Loss: 0.141674 Acc: 0.988715 0.98884 0.991212653779 0.991212653779\n",
      "Loss: 0.141673 Acc: 0.98884 0.988953 0.991212653779 0.991212653779\n",
      "Loss: 0.141671 Acc: 0.988953 0.989056 0.991212653779 0.991212653779\n",
      "Loss: 0.14167 Acc: 0.989056 0.98915 0.991212653779 0.991212653779\n",
      "Loss: 0.141668 Acc: 0.98915 0.989236 0.991212653779 0.991212653779\n",
      "Loss: 0.141667 Acc: 0.989236 0.989315 0.991212653779 0.991212653779\n",
      "Loss: 0.141666 Acc: 0.989315 0.989388 0.991212653779 0.991212653779\n",
      "Loss: 0.141665 Acc: 0.989388 0.989455 0.991212653779 0.991212653779\n",
      "Loss: 0.141664 Acc: 0.989455 0.989518 0.991212653779 0.991212653779\n",
      "Loss: 0.141663 Acc: 0.989518 0.989576 0.991212653779 0.991212653779\n",
      "Loss: 0.141662 Acc: 0.989576 0.989631 0.991212653779 0.991212653779\n",
      "Loss: 0.141661 Acc: 0.989631 0.989682 0.991212653779 0.991212653779\n",
      "Loss: 0.141661 Acc: 0.989682 0.98973 0.991212653779 0.991212653779\n",
      "Loss: 0.14166 Acc: 0.98973 0.989775 0.991212653779 0.991212653779\n",
      "Loss: 0.141659 Acc: 0.989775 0.989817 0.991212653779 0.991212653779\n",
      "Loss: 0.141659 Acc: 0.989817 0.989857 0.991212653779 0.991212653779\n",
      "Loss: 0.141658 Acc: 0.989857 0.989895 0.991212653779 0.991212653779\n",
      "Loss: 0.141657 Acc: 0.989895 0.98993 0.991212653779 0.991212653779\n",
      "Loss: 0.141657 Acc: 0.98993 0.989964 0.991212653779 0.991212653779\n",
      "Loss: 0.141656 Acc: 0.989964 0.989996 0.991212653779 0.991212653779\n",
      "Loss: 0.141656 Acc: 0.989996 0.990026 0.991212653779 0.991212653779\n",
      "Loss: 0.141655 Acc: 0.990026 0.990055 0.991212653779 0.991212653779\n",
      "Loss: 0.141655 Acc: 0.990055 0.990083 0.991212653779 0.991212653779\n",
      "Loss: 0.141655 Acc: 0.990083 0.990109 0.991212653779 0.991212653779\n",
      "Loss: 0.141654 Acc: 0.990109 0.990134 0.991212653779 0.991212653779\n",
      "Loss: 0.141654 Acc: 0.990134 0.990158 0.991212653779 0.991212653779\n",
      "Loss: 0.141653 Acc: 0.990158 0.990181 0.991212653779 0.991212653779\n",
      "Loss: 0.141653 Acc: 0.990181 0.990203 0.991212653779 0.991212653779\n",
      "Loss: 0.141653 Acc: 0.990203 0.990224 0.991212653779 0.991212653779\n",
      "Loss: 0.141652 Acc: 0.990224 0.990244 0.991212653779 0.991212653779\n",
      "Loss: 0.141652 Acc: 0.990244 0.990264 0.991212653779 0.991212653779\n",
      "Loss: 0.141652 Acc: 0.990264 0.990282 0.991212653779 0.991212653779\n",
      "Loss: 0.141652 Acc: 0.0 0.991213\n",
      "[[-14.84865081]\n",
      " [-14.82709408]\n",
      " [-14.37653245]\n",
      " [-15.87809844]\n",
      " [-11.36651867]\n",
      " [ -2.34438601]\n",
      " [-17.0147418 ]\n",
      " [-19.62320339]\n",
      " [ -3.87847851]\n",
      " [  4.24117316]\n",
      " [-23.05073202]\n",
      " [  6.96500613]\n",
      " [-17.05150376]\n",
      " [-20.82822934]\n",
      " [ -1.79467527]\n",
      " [ 17.56258243]\n",
      " [  0.3901013 ]\n",
      " [ -6.21263754]\n",
      " [  9.77703467]\n",
      " [ 16.20733199]\n",
      " [-22.09962873]\n",
      " [-21.81307935]\n",
      " [-21.14786964]\n",
      " [-24.70179225]\n",
      " [-18.14584026]\n",
      " [ -6.22495583]\n",
      " [-16.54024302]\n",
      " [-23.55426473]\n",
      " [-14.59311769]\n",
      " [ -0.0731984 ]]\n",
      "[ 15.72133858]\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     lcl.run()\n",
    "     sess.run(training_op)\n",
    "     #print np.c_[prob_positive.eval(), jj.eval(), hh.eval(), vv.eval()]\n",
    "     print prob_positive.eval().min(), prob_positive.eval().max()\n",
    "     for epoch in xrange(n_epochs + 1): \n",
    "          sess.run(training_op)\n",
    "          if not (epoch % 100):\n",
    "               print \"Loss:\", loss.eval(), \"Acc:\", accuracy.eval(), accuracy_op.eval(), acc.eval() / float(X_std.shape[0]), acc2.eval() / float(X_std.shape[0])\n",
    "     if not (epoch % 100): print \"Loss:\", loss.eval(), \"Acc:\", accuracy2.eval(), accuracy_op2.eval()\n",
    "\n",
    "     print tf.get_default_graph().get_tensor_by_name('single_neuron/kernel:0').eval()\n",
    "     print tf.get_default_graph().get_tensor_by_name('single_neuron/bias:0').eval()\n",
    "     print prob_positive.eval().min(), prob_positive.eval().max()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Monday, November 6, 2017\n",
    "# Scitkit-Learn MLP versus Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLPClassifier is a fully connected neural network. It offers choices of the activation function as well as L2 regularization. There are a large number of parameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(load_digits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_digits().data, load_digits().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   5.,  14.,  14.,   2.,   0.,   0.,   0.,   2.,  16.,\n",
       "        16.,  16.,   7.,   0.,   0.,   0.,   0.,   7.,   4.,  16.,  12.,\n",
       "         0.,   0.,   0.,   0.,   1.,   9.,  16.,  16.,   8.,   0.,   0.,\n",
       "         3.,  15.,  16.,  16.,  10.,   2.,   0.,   0.,   4.,  16.,  16.,\n",
       "        11.,   0.,   0.,   0.,   0.,   0.,   9.,  16.,   5.,   0.,   0.,\n",
       "         0.,   0.,   0.,   9.,  13.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 0, ..., 3, 1, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std_sc = StandardScaler()\n",
    "X_train_std = std_sc.fit_transform(X_train).astype(np.float32)\n",
    "X_test_std = std_sc.transform(X_test).astype(np.float32)\n",
    "mm_sc = MinMaxScaler()\n",
    "X_train_mm = mm_sc.fit_transform(X_train).astype(np.float32)\n",
    "X_test_mm = mm_sc.transform(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mm.min(), X_train_mm.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0, batch_size=200, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting early_stopping to True made the model worse\n",
    "# setting alpha to 1000 produces poor results (too much regularization)\n",
    "# note that alpha is the prefactor for L2 regularization of the neuron weights\n",
    "mlp_clf = MLPClassifier(activation='relu', batch_size=200, hidden_layer_sizes=(100, 50),\n",
    "                        learning_rate='constant', early_stopping=False, alpha=0.0, max_iter=400)\n",
    "mlp_clf.fit(X_train_mm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99512874043145438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(mlp_clf.predict(X_train_mm), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.95833333333333337, 0.97769677983600933, 0.99305555555555558)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(mlp_clf, X_train_mm, y_train, cv=10)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(mlp_clf.predict(X_test_mm), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 64\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "     hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "     hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "     logits  = tf.layers.dense(hidden2, n_outputs, name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "     loss = tf.reduce_mean(xentropy, name='loss')\n",
    "with tf.name_scope('train'):\n",
    "     optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "     training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope('eval'):\n",
    "     correct = tf.nn.in_top_k(logits, y, 1)\n",
    "     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(batch_size):\n",
    "     indices = np.random.choice(range(X_train_mm.shape[0]), size=batch_size, replace=False)\n",
    "     return X_train_mm[indices], y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train accuracy:', 0.039999999, 'Test accuracy:', 0.03888889)\n",
      "(100, 'Train accuracy:', 0.91500002, 'Test accuracy:', 0.875)\n",
      "(200, 'Train accuracy:', 0.94999999, 'Test accuracy:', 0.92500001)\n",
      "(300, 'Train accuracy:', 0.95499998, 'Test accuracy:', 0.92777777)\n",
      "(400, 'Train accuracy:', 0.95999998, 'Test accuracy:', 0.94444442)\n",
      "(500, 'Train accuracy:', 0.97000003, 'Test accuracy:', 0.94999999)\n",
      "(600, 'Train accuracy:', 0.98000002, 'Test accuracy:', 0.9527778)\n",
      "(700, 'Train accuracy:', 0.98000002, 'Test accuracy:', 0.95555556)\n",
      "(800, 'Train accuracy:', 0.99000001, 'Test accuracy:', 0.96388888)\n",
      "(900, 'Train accuracy:', 0.98500001, 'Test accuracy:', 0.96944445)\n",
      "(1000, 'Train accuracy:', 0.99000001, 'Test accuracy:', 0.96666664)\n",
      "(1100, 'Train accuracy:', 0.98000002, 'Test accuracy:', 0.96944445)\n",
      "(1200, 'Train accuracy:', 0.98000002, 'Test accuracy:', 0.96944445)\n",
      "(1300, 'Train accuracy:', 1.0, 'Test accuracy:', 0.96944445)\n",
      "(1400, 'Train accuracy:', 0.98500001, 'Test accuracy:', 0.96944445)\n",
      "(1500, 'Train accuracy:', 0.99000001, 'Test accuracy:', 0.96944445)\n",
      "(1600, 'Train accuracy:', 0.995, 'Test accuracy:', 0.96944445)\n",
      "(1700, 'Train accuracy:', 1.0, 'Test accuracy:', 0.97222221)\n",
      "(1800, 'Train accuracy:', 1.0, 'Test accuracy:', 0.97222221)\n",
      "(1900, 'Train accuracy:', 1.0, 'Test accuracy:', 0.97500002)\n",
      "(2000, 'Train accuracy:', 0.995, 'Test accuracy:', 0.97222221)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "batch_size = 200\n",
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for epoch in xrange(n_epochs + 1):\n",
    "          for iteration in xrange(X_train_mm.shape[0] // batch_size):\n",
    "               X_batch, y_batch = fetch_batch(batch_size)\n",
    "               sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "          acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "          acc_test = accuracy.eval(feed_dict={X:X_test_mm, y:y_test})\n",
    "          if (epoch % 100 == 0): print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hs = pd.read_csv('../machine_learning/geron_housing/housing.csv', header=0)\n",
    "hs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      "longitude             20640 non-null float64\n",
      "latitude              20640 non-null float64\n",
      "housing_median_age    20640 non-null float64\n",
      "total_rooms           20640 non-null float64\n",
      "total_bedrooms        20433 non-null float64\n",
      "population            20640 non-null float64\n",
      "households            20640 non-null float64\n",
      "median_income         20640 non-null float64\n",
      "median_house_value    20640 non-null float64\n",
      "ocean_proximity       20640 non-null object\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "hs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hs.drop('ocean_proximity', axis=1, inplace=True)\n",
    "hs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20433, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = hs.median_house_value.values\n",
    "X = hs.drop('median_house_value', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std_hs_X = StandardScaler()\n",
    "X_train_std = std_hs_X.fit_transform(X_train).astype(np.float32)\n",
    "X_test_std = std_hs_X.transform(X_test).astype(np.float32)\n",
    "std_hs_y = StandardScaler()\n",
    "y_train_std = std_hs_y.fit_transform(y_train.reshape(-1, 1)).ravel().astype(np.float32)\n",
    "y_test_std = std_hs_y.transform(y_test.reshape(-1, 1)).ravel().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79238302, -0.81553048,  0.34024277, -0.21641906, -0.36556655,\n",
       "       -0.16907337, -0.30344927,  0.95104498], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03138348,  1.06341052,  1.42457962, ..., -0.42293945,\n",
       "        1.24659967, -1.026335  ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor(n_estimators=100)\n",
    "rf_reg.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.025658488955372781, 0.97434151064336516)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train_std, rf_reg.predict(X_train_std)), r2_score(y_train_std, rf_reg.predict(X_train_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18031021351509532, 0.18911377613651628, 0.19698436577569756)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = -cross_val_score(rf_reg, X_train_std, y_train_std, cv=5, scoring='neg_mean_squared_error')\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80361579044393572, 0.8106230063291211, 0.81802947759990563)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(rf_reg, X_train_std, y_train_std, cv=5, scoring='r2')\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_reg = MLPRegressor(activation='relu', batch_size=200, hidden_layer_sizes=(100, 50),\n",
    "                       learning_rate='constant', early_stopping=False, alpha=0.0, max_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0, batch_size=200, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_reg.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18142764337538192, 0.81857235378734938)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train_std, mlp_reg.predict(X_train_std)), r2_score(y_train_std, mlp_reg.predict(X_train_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorflow with batch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_outputs = 1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "output  = tf.layers.dense(inputs=hidden2, units=n_outputs, name='outputs', activation=None)\n",
    "y_pred = tf.squeeze(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = tf.losses.mean_squared_error(y, y_pred)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train mse:', 2.2199144, 'Test mse:', 2.2694972)\n",
      "(200, 'Train mse:', 0.25983775, 'Test mse:', 0.27191079)\n",
      "(400, 'Train mse:', 0.24855487, 'Test mse:', 0.2613675)\n",
      "(600, 'Train mse:', 0.24075039, 'Test mse:', 0.25349453)\n",
      "(800, 'Train mse:', 0.23483436, 'Test mse:', 0.24736236)\n",
      "(1000, 'Train mse:', 0.23009399, 'Test mse:', 0.242815)\n",
      "(1200, 'Train mse:', 0.2261721, 'Test mse:', 0.23941334)\n",
      "(1400, 'Train mse:', 0.22495337, 'Test mse:', 0.23902963)\n",
      "(1600, 'Train mse:', 0.2214569, 'Test mse:', 0.23615052)\n",
      "(1800, 'Train mse:', 0.21876733, 'Test mse:', 0.23373525)\n",
      "(2000, 'Train mse:', 0.21809483, 'Test mse:', 0.23386538)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for epoch in range(n_epochs + 1):\n",
    "          sess.run(training_op, feed_dict={X:X_train_std, y:y_train_std})\n",
    "          if (epoch % 200 == 0):\n",
    "               mse_train = mse.eval(feed_dict={X:X_train_std, y:y_train_std})\n",
    "               mse_test = mse.eval(feed_dict={X:X_test_std, y:y_test_std})\n",
    "               print(epoch, \"Train mse:\", mse_train, \"Test mse:\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow with mini-batch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "     hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "     hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "     output  = tf.layers.dense(inputs=hidden2, units=n_outputs, name='outputs', activation=None)\n",
    "     y_pred  = tf.squeeze(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('mse'):\n",
    "     mse = tf.losses.mean_squared_error(y, y_pred)\n",
    "     r2 = 1.0 - tf.reduce_mean(tf.square(y - y_pred)) / tf.reduce_mean(tf.square(y - tf.reduce_mean(y)))\n",
    "with tf.name_scope('train'):\n",
    "     optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "     training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch2(batch_size_):\n",
    "     indices = np.random.choice(range(X_train_std.shape[0]), size=batch_size_, replace=False)\n",
    "     return X_train_std[indices], y_train_std[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Train mse:', 0.39633209, 'Test mse:', 0.34032384, 'R2:', 0.66685188)\n",
      "(100, 'Train mse:', 0.18098667, 'Test mse:', 0.20702285, 'R2:', 0.79734218)\n",
      "(200, 'Train mse:', 0.17564258, 'Test mse:', 0.2092379, 'R2:', 0.79517382)\n",
      "(300, 'Train mse:', 0.13212143, 'Test mse:', 0.1996471, 'R2:', 0.80456245)\n",
      "(400, 'Train mse:', 0.15911207, 'Test mse:', 0.19874652, 'R2:', 0.805444)\n",
      "(500, 'Train mse:', 0.12621102, 'Test mse:', 0.20320998, 'R2:', 0.80107468)\n",
      "(600, 'Train mse:', 0.14272358, 'Test mse:', 0.20400697, 'R2:', 0.80029452)\n",
      "(700, 'Train mse:', 0.1131544, 'Test mse:', 0.20746927, 'R2:', 0.79690516)\n",
      "(800, 'Train mse:', 0.096681148, 'Test mse:', 0.20730844, 'R2:', 0.79706264)\n",
      "(900, 'Train mse:', 0.099172764, 'Test mse:', 0.20997347, 'R2:', 0.7944538)\n",
      "(1000, 'Train mse:', 0.090603665, 'Test mse:', 0.21003853, 'R2:', 0.79439008)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 200\n",
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for epoch in xrange(n_epochs + 1):\n",
    "          for iteration in xrange(X_train_std.shape[0] // batch_size):\n",
    "               X_batch, y_batch = fetch_batch2(batch_size)\n",
    "               sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "          mse_train = mse.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "          mse_test = mse.eval(feed_dict={X:X_test_std, y:y_test_std})\n",
    "          r2_test = r2.eval(feed_dict={X:X_test_std, y:y_test_std})\n",
    "          if (epoch % 100 == 0):\n",
    "               print(epoch, \"Train mse:\", mse_train, \"Test mse:\", mse_test, \"R2:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorflow from contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a305fa890>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/tz/vmnmfncn05xfhb89h953_8_w0000gn/T/tmpLz4FaS\n",
      "WARNING:tensorflow:From /Users/jhalverson/software/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/tz/vmnmfncn05xfhb89h953_8_w0000gn/T/tmpLz4FaS/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.13595, step = 1\n",
      "INFO:tensorflow:global_step/sec: 407.683\n",
      "INFO:tensorflow:loss = 0.258838, step = 101 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 414.995\n",
      "INFO:tensorflow:loss = 0.22243, step = 201 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.789\n",
      "INFO:tensorflow:loss = 0.247194, step = 301 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.391\n",
      "INFO:tensorflow:loss = 0.209024, step = 401 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.605\n",
      "INFO:tensorflow:loss = 0.319272, step = 501 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.918\n",
      "INFO:tensorflow:loss = 0.371682, step = 601 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.177\n",
      "INFO:tensorflow:loss = 0.248293, step = 701 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.307\n",
      "INFO:tensorflow:loss = 0.306317, step = 801 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.133\n",
      "INFO:tensorflow:loss = 0.225206, step = 901 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.235\n",
      "INFO:tensorflow:loss = 0.21714, step = 1001 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.283\n",
      "INFO:tensorflow:loss = 0.267, step = 1101 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.352\n",
      "INFO:tensorflow:loss = 0.281446, step = 1201 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 413.02\n",
      "INFO:tensorflow:loss = 0.318997, step = 1301 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.152\n",
      "INFO:tensorflow:loss = 0.261012, step = 1401 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.701\n",
      "INFO:tensorflow:loss = 0.171399, step = 1501 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.554\n",
      "INFO:tensorflow:loss = 0.269857, step = 1601 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.413\n",
      "INFO:tensorflow:loss = 0.175262, step = 1701 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.846\n",
      "INFO:tensorflow:loss = 0.157738, step = 1801 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.175\n",
      "INFO:tensorflow:loss = 0.227566, step = 1901 (0.244 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/tz/vmnmfncn05xfhb89h953_8_w0000gn/T/tmpLz4FaS/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.16095.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tf.estimator.DNNRegressor with version 1.4\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train_std)\n",
    "dnn_reg = tf.contrib.learn.DNNRegressor(hidden_units=[100, 50], feature_columns=feature_cols)\n",
    "dnn_reg = tf.contrib.learn.SKCompat(dnn_reg)\n",
    "dnn_reg.fit(X_train_std, y_train_std, batch_size=200, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tz/vmnmfncn05xfhb89h953_8_w0000gn/T/tmpLz4FaS/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_reg.predict(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20901635, 0.79098364127145637)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train_std, y_pred['scores']), r2_score(y_train_std, y_pred['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tz/vmnmfncn05xfhb89h953_8_w0000gn/T/tmpLz4FaS/model.ckpt-2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2245321, 0.78020218241586825)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = dnn_reg.predict(X_test_std)\n",
    "mean_squared_error(y_test_std, y_pred_test['scores']), r2_score(y_test_std, y_pred_test['scores'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

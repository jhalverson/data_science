{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Wednesday, January 17, 2018\n",
    "# Melville versus Austen: With word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train an RNN to classify sentences as written by either Herman Melville or Jane Austen. We have 12,500 sentences between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melville_raw = list(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "austen_raw = list(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
    "austen_raw2 = list(nltk.corpus.gutenberg.words('austen-persuasion.txt'))\n",
    "austen_raw3 = list(nltk.corpus.gutenberg.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melville = melville_raw[melville_raw.index('Loomings'):]\n",
    "austen = austen_raw[austen_raw.index('The'):]\n",
    "austen2 = austen_raw2[austen_raw2.index('Sir'):]\n",
    "austen3 = austen_raw3[austen_raw3.index('I'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('melville_pierre.txt') as f:\n",
    "     melville2 = f.read().decode('utf-8').encode('ascii', 'ignore').replace('\\n', ' ').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word embeddings as generated by word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word2vec-master/vectors.text', 'r') as f:\n",
    "     lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "for line in lines[1:]:\n",
    "     word_nums = line.split()\n",
    "     word = word_nums[0]\n",
    "     word_vectors[word] = np.array(map(float, word_nums[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = int(lines[0].split()[1])\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sentences(x):\n",
    "     j = ' '.join(x).replace('Mr .', 'Mr').replace('Mrs .', 'Mrs')\n",
    "     j = j.replace('Ms .', 'Ms').replace('Dr .', 'Dr').replace('\\n', ' ')\n",
    "     j = j.replace('?', '.').replace('!', '.').replace('CHAPTER', ' ')\n",
    "     sentences = j.split('.')\n",
    "     s = [re.sub(\"[^a-zA-Z]\", \" \", sentence) for sentence in sentences]\n",
    "     s = [sentence.lower().split() for sentence in s]\n",
    "     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_single_letters_except_ia(sentences):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          cleaned_sentence = []\n",
    "          for word in sentence:\n",
    "               if len(word) > 1:\n",
    "                    cleaned_sentence.append(word)\n",
    "               else:\n",
    "                    if word in ['a', 'i']:\n",
    "                         cleaned_sentence.append(word)\n",
    "          new_sentences.append(cleaned_sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_short_and_long_sentences(sentences, low, high):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          if (len(sentence) >= low and len(sentence) <= high):\n",
    "               new_sentences.append(sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_missing_words_with_UNK(sentences, missing):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          cleaned_sentence = []\n",
    "          for word in sentence:\n",
    "               if word in missing:\n",
    "                    cleaned_sentence.append('</s>')\n",
    "               else:\n",
    "                    cleaned_sentence.append(word)\n",
    "          new_sentences.append(cleaned_sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_word_with_index_and_zero_pad(sentences, dictionary, high):\n",
    "     number_sentences = []\n",
    "     for sentence in sentences:\n",
    "          # how to handle words not in vocabulary\n",
    "          number_sentence = [dictionary[word] for word in sentence]\n",
    "          for _ in range(high - len(number_sentence)):\n",
    "               number_sentence.append(0)\n",
    "          number_sentences.append(number_sentence)\n",
    "     return number_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = remove_single_letters_except_ia(make_sentences(melville)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(melville2))\n",
    "s2 = remove_single_letters_except_ia(make_sentences(austen)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(austen2)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(austen3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_bound = 15\n",
    "s1 = remove_short_and_long_sentences(s1, 5, upper_bound)\n",
    "s2 = remove_short_and_long_sentences(s2, 5, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing words with 'ukn':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247 7298\n"
     ]
    }
   ],
   "source": [
    "print len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = np.array([len(sentence) for sentence in s1] + [len(sentence) for sentence in s2])\n",
    "target = np.append(np.ones(len(s1)), np.zeros(len(s2))).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = [word for sentence in s1 for word in sentence] + \\\n",
    "            [word for sentence in s2 for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119046"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9888"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = set(all_words + ['</s>'])\n",
    "vocabulary_size = len(unique_words)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_word_vector_words = set(word_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1949\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for word in unique_words:\n",
    "     if word not in unique_word_vector_words:\n",
    "          ct += 1\n",
    "print ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing = unique_words - unique_word_vector_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = replace_missing_words_with_UNK(s1, missing)\n",
    "s2 = replace_missing_words_with_UNK(s2, missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = dict([(word, index) for index, word in enumerate(unique_words - missing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sentences = replace_word_with_index_and_zero_pad(s1, dictionary, high=upper_bound) + \\\n",
    "                replace_word_with_index_and_zero_pad(s2, dictionary, high=upper_bound)\n",
    "all_sentences = np.array(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.arange(target.size)\n",
    "np.random.shuffle(idx)\n",
    "all_sentences = all_sentences[idx]\n",
    "target = target[idx]\n",
    "seq_length = seq_length[idx]\n",
    "\n",
    "\n",
    "#seq_length = np.array(seq_length.size * [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "idx_cut = int((1.0 - test_size) * target.size)\n",
    "X_training = all_sentences[:idx_cut]\n",
    "X_test = all_sentences[idx_cut:]\n",
    "y_training = target[:idx_cut]\n",
    "y_test = target[idx_cut:]\n",
    "L_training = seq_length[:idx_cut]\n",
    "L_test = seq_length[idx_cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_training_batch(A, b, c, batch_size):\n",
    "     idx = np.random.choice(np.arange(y_training.size), size=batch_size, replace=False)\n",
    "     return A[idx], b[idx], c[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "n_inputs = embedding_size\n",
    "n_steps = 15\n",
    "n_neurons = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, shape=(None, n_steps))\n",
    "y = tf.placeholder(tf.int32, shape=(None))\n",
    "L = tf.placeholder(dtype=tf.int32, shape=(None))\n",
    "training = tf.placeholder_with_default(False, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in word_vectors.keys() if '<' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.293899368440778"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(word_vectors['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4046846819093144"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.random.random(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverse_dictionary = dict([(index, word) for (word, index) in dictionary.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'yellow'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.136894, -0.037584, -0.352259])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['yellow'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35946757265379703"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(word_vectors['king'], word_vectors['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3669084423242156"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "distance.cosine(q, word_vectors['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ee = []\n",
    "for index in xrange(len(inverse_dictionary.keys())):\n",
    "     ee.append(word_vectors[inverse_dictionary[index]])\n",
    "embeddings_matrix = np.array(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7939, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.constant(embeddings_matrix, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=True)\n",
    "embed = tf.nn.embedding_lookup(params=embeddings, ids=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32, sequence_length=L)\n",
    "fc_drop = tf.layers.dropout(states, rate=0.9, training=training)\n",
    "logits_2d = tf.layers.dense(fc_drop, units=1, activation=None)\n",
    "logits = tf.squeeze(logits_2d)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32), logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.greater(logits, 0.0), tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y, y_pred), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 75\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.266384 0.893333 0.397742 0.808689\n",
      "5 0.136174 0.973333 0.334062 0.851335\n",
      "10 0.172522 0.933333 0.394252 0.843364\n",
      "15 0.127781 0.96 0.395126 0.852531\n",
      "20 0.138739 0.96 0.523594 0.848944\n",
      "25 0.0845923 0.96 0.483823 0.852531\n",
      "30 0.112889 0.946667 0.498843 0.850937\n",
      "35 0.120944 0.96 0.465441 0.849342\n",
      "40 0.223942 0.96 0.451217 0.850538\n",
      "45 0.0804978 0.973333 0.446511 0.854922\n",
      "50 0.210791 0.946667 0.393126 0.834994\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for epoch in xrange(epochs + 1):\n",
    "          for iteration in xrange(y_training.size // batch_size):\n",
    "               X_batch, y_batch, L_batch = fetch_training_batch(X_training, y_training, L_training, batch_size)\n",
    "               sess.run(training_op, feed_dict={X:X_batch, y:y_batch, L:L_batch, training:True})\n",
    "          if epoch % 5 == 0:\n",
    "               loss_batch, acc_batch = sess.run([loss, accuracy], feed_dict={X:X_batch, y:y_batch, L:L_batch, training:False})\n",
    "               loss_test, acc_test = sess.run([loss, accuracy], feed_dict={X:X_test, y:y_test, L:L_test, training:False})\n",
    "               print epoch, loss_batch, acc_batch, loss_test, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n_neurons|embedding_size|embeddings_trainable|dropout_rate|dropout_during_testing|sequence|peak accuracy|\n",
    "|------|------|------|------|------|------|\n",
    "| 2 | 200| no| 0.9|no|var|81.0%|\n",
    "| 64 | 200| no| 0.9|no|var|85.4%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word2vec word embeddings give similar results to trainable word embeddings from the previous notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

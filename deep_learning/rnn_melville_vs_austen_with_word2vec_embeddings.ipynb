{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Wednesday, January 17, 2018\n",
    "# Melville versus Austen: With word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train an RNN to classify sentences as written by either Herman Melville or Jane Austen. We have 12,500 sentences between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melville_raw = list(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "austen_raw = list(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
    "austen_raw2 = list(nltk.corpus.gutenberg.words('austen-persuasion.txt'))\n",
    "austen_raw3 = list(nltk.corpus.gutenberg.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melville = melville_raw[melville_raw.index('Loomings'):]\n",
    "austen = austen_raw[austen_raw.index('The'):]\n",
    "austen2 = austen_raw2[austen_raw2.index('Sir'):]\n",
    "austen3 = austen_raw3[austen_raw3.index('I'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('melville_pierre.txt') as f:\n",
    "     melville2 = f.read().decode('utf-8').encode('ascii', 'ignore').replace('\\n', ' ').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word embeddings as generated by word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word2vec-master/vectors.text', 'r') as f:\n",
    "     lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "for line in lines[1:]:\n",
    "     word_nums = line.split()\n",
    "     word = word_nums[0]\n",
    "     word_vectors[word] = np.array(map(float, word_nums[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = int(lines[0].split()[1])\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sentences(x):\n",
    "     j = ' '.join(x).replace('Mr .', 'Mr').replace('Mrs .', 'Mrs')\n",
    "     j = j.replace('Ms .', 'Ms').replace('Dr .', 'Dr').replace('\\n', ' ')\n",
    "     j = j.replace('?', '.').replace('!', '.').replace('CHAPTER', ' ')\n",
    "     sentences = j.split('.')\n",
    "     s = [re.sub(\"[^a-zA-Z]\", \" \", sentence) for sentence in sentences]\n",
    "     s = [sentence.lower().split() for sentence in s]\n",
    "     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_single_letters_except_ia(sentences):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          cleaned_sentence = []\n",
    "          for word in sentence:\n",
    "               if len(word) > 1:\n",
    "                    cleaned_sentence.append(word)\n",
    "               else:\n",
    "                    if word in ['a', 'i']:\n",
    "                         cleaned_sentence.append(word)\n",
    "          new_sentences.append(cleaned_sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_short_and_long_sentences(sentences, low, high):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          if (len(sentence) >= low and len(sentence) <= high):\n",
    "               new_sentences.append(sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_missing_words_with_UNK(sentences, missing):\n",
    "     new_sentences = []\n",
    "     for sentence in sentences:\n",
    "          cleaned_sentence = []\n",
    "          for word in sentence:\n",
    "               if word in missing:\n",
    "                    cleaned_sentence.append('[UNK]')\n",
    "               else:\n",
    "                    cleaned_sentence.append(word)\n",
    "          new_sentences.append(cleaned_sentence)\n",
    "     return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_word_with_index_and_zero_pad(sentences, dictionary, high):\n",
    "     number_sentences = []\n",
    "     for sentence in sentences:\n",
    "          # how to handle words not in vocabulary\n",
    "          number_sentence = [dictionary[word] for word in sentence]\n",
    "          for _ in range(high - len(number_sentence)):\n",
    "               number_sentence.append(0)\n",
    "          number_sentences.append(number_sentence)\n",
    "     return number_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = remove_single_letters_except_ia(make_sentences(melville)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(melville2))\n",
    "s2 = remove_single_letters_except_ia(make_sentences(austen)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(austen2)) + \\\n",
    "     remove_single_letters_except_ia(make_sentences(austen3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_bound = 15\n",
    "s1 = remove_short_and_long_sentences(s1, 5, upper_bound)\n",
    "s2 = remove_short_and_long_sentences(s2, 5, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing words with 'ukn':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247 7298\n"
     ]
    }
   ],
   "source": [
    "print len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = np.array([len(sentence) for sentence in s1] + [len(sentence) for sentence in s2])\n",
    "target = np.append(np.ones(len(s1)), np.zeros(len(s2))).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = [word for sentence in s1 for word in sentence] + \\\n",
    "            [word for sentence in s2 for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119046"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9888"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = set(all_words + ['[UNK]'])\n",
    "vocabulary_size = len(unique_words)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_word_vector_words = set(word_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for word in unique_word_vector_words:\n",
    "     if word not in w2v:\n",
    "          ct += 1\n",
    "print ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing = unique_words - unique_word_vector_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = replace_missing_words_with_UNK(s1, missing)\n",
    "s2 = replace_missing_words_with_UNK(s2, missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = dict([(word, index) for index, word in enumerate(unique_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sentences = replace_word_with_index_and_zero_pad(s1, dictionary, high=upper_bound) + \\\n",
    "                replace_word_with_index_and_zero_pad(s2, dictionary, high=upper_bound)\n",
    "all_sentences = np.array(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.arange(target.size)\n",
    "np.random.shuffle(idx)\n",
    "all_sentences = all_sentences[idx]\n",
    "target = target[idx]\n",
    "seq_length = seq_length[idx]\n",
    "\n",
    "\n",
    "#seq_length = np.array(seq_length.size * [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "idx_cut = int((1.0 - test_size) * target.size)\n",
    "X_training = all_sentences[:idx_cut]\n",
    "X_test = all_sentences[idx_cut:]\n",
    "y_training = target[:idx_cut]\n",
    "y_test = target[idx_cut:]\n",
    "L_training = seq_length[:idx_cut]\n",
    "L_test = seq_length[idx_cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_training_batch(A, b, c, batch_size):\n",
    "     idx = np.random.choice(np.arange(y_training.size), size=batch_size, replace=False)\n",
    "     return A[idx], b[idx], c[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 2\n",
    "n_inputs = embedding_size\n",
    "n_steps = 15\n",
    "n_neurons = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, shape=(None, n_steps))\n",
    "y = tf.placeholder(tf.int32, shape=(None))\n",
    "L = tf.placeholder(dtype=tf.int32, shape=(None))\n",
    "training = tf.placeholder_with_default(False, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5746"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in word_vectors.keys() if '<' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.293899368440778"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(word_vectors['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3341123639300001"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.random.random(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding = tf.get_variable('embeddings_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = tf.constant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=True)\n",
    "embed = tf.nn.embedding_lookup(params=embeddings, ids=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32, sequence_length=L)\n",
    "fc_drop = tf.layers.dropout(states, rate=0.9, training=training)\n",
    "logits_2d = tf.layers.dense(fc_drop, units=1, activation=None)\n",
    "logits = tf.squeeze(logits_2d)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y, tf.float32), logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.greater(logits, 0.0), tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y, y_pred), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 75\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.700189 0.52 0.67223 0.599841\n",
      "5 0.684881 0.533333 0.657825 0.611798\n",
      "10 0.697682 0.52 0.641595 0.639697\n",
      "15 0.658339 0.56 0.641446 0.646871\n",
      "20 0.692562 0.48 0.637728 0.639298\n",
      "25 0.643722 0.573333 0.64042 0.640096\n",
      "30 0.586242 0.773333 0.640402 0.642886\n",
      "35 0.617598 0.706667 0.634951 0.643683\n",
      "40 0.613218 0.706667 0.630088 0.645676\n",
      "45 0.62445 0.706667 0.63577 0.642886\n",
      "50 0.618031 0.64 0.634087 0.64448\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     for epoch in xrange(epochs + 1):\n",
    "          for iteration in xrange(y_training.size // batch_size):\n",
    "               X_batch, y_batch, L_batch = fetch_training_batch(X_training, y_training, L_training, batch_size)\n",
    "               sess.run(training_op, feed_dict={X:X_batch, y:y_batch, L:L_batch, training:True})\n",
    "          if epoch % 5 == 0:\n",
    "               loss_batch, acc_batch = sess.run([loss, accuracy], feed_dict={X:X_batch, y:y_batch, L:L_batch, training:False})\n",
    "               loss_test, acc_test = sess.run([loss, accuracy], feed_dict={X:X_test, y:y_test, L:L_test, training:False})\n",
    "               print epoch, loss_batch, acc_batch, loss_test, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|n_neurons|embedding_size|embeddings_trainable|dropout_rate|dropout_during_testing|sequence|peak accuracy|\n",
    "|------|------|------|------|------|------|\n",
    "| 64 | 64| yes| 0.9|no|var|87.0%|\n",
    "| 64 | 64| yes| 0.9|yes|var|85.6%|\n",
    "| 64 | 64| no| 0.9|no|var|79.9%|\n",
    "| 32 | 32| yes| 0.9|no|var|87.2%|\n",
    "| 16 | 16| yes| 0.9|no|var|86.2%|\n",
    "| 8 | 8| yes| 0.9|no|var|87.4%|\n",
    "| 2 | 2| yes| 0.9|no|var|86.7%|\n",
    "| 32 | 32| yes| 0.9|no|all 25|86.1%|\n",
    "| 32 | 32| yes| 0.9|no|all 5|79.7%|\n",
    "| 32 | 32| yes| 0.9|no|all 3|76.3%|\n",
    "| 32 | 32| yes| 0.9|no|all 1|68.8%|\n",
    "| 2 | 2| yes| 0.9|no|all 1|64.5%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model with the best hyperparameters gives about 86% accuracy in distinguishing between Melville and Austen. This seems reasonable given that some sentences have only five words. Our results indicate that using just the first word of each sentence and only two neurons leads to better than random guessing results. In fact we get essentially the same results when using 2 or 32 neurons (and an embedding size of 2 or 32). The network is subject to overfitting so we using dropout and by reporting the peak accuracy we are essentially using early stopping.\n",
    "\n",
    "When the sequence length is set to 5 the accuracy drops a few percent. When it is set to one the accuracy is drops significantly but it is still better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6157831805500199"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier(n_estimators=25).fit(X_training, y_training).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't make much sense given the zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "seq_length = tf.placeholder(tf.int32, (None))\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "# this a general way of obtaining the last output (and may be different to states (e.g., stacked cells))\n",
    "idx = tf.range(4) * tf.shape(output_seqs)[1] + (seq_length - 1)\n",
    "last_rnn_output = tf.gather(tf.reshape(output_seqs, [-1, n_neurons]), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "        # t = 0      t = 1 \n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [1e6, 1e6, 1e6]], # instance 2\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "seq_length_batch = [2, 1, 2, 2]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "     init.run()\n",
    "     outputs_val = output_seqs.eval(feed_dict={X: X_batch, seq_length:seq_length_batch})\n",
    "     states_val = states.eval(feed_dict={X: X_batch, seq_length:seq_length_batch})\n",
    "     last_rnn_output = states.eval(feed_dict={X: X_batch, seq_length:seq_length_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.642093   -0.69863158 -0.03004719  0.31377819  0.4477174 ]\n",
      "  [ 0.99999917 -0.99999917  0.88607311  0.7366519   0.4490065 ]]\n",
      "\n",
      " [[ 0.99804538 -0.99902999  0.28130701  0.68677801  0.68291724]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.99999142 -0.9999975   0.54289955  0.8761453   0.82973319]\n",
      "  [ 0.99971676 -0.99965465  0.62960362  0.28283864 -0.40961957]]\n",
      "\n",
      " [[ 0.99999028 -0.98174554 -0.64455634 -0.99006081 -0.87356085]\n",
      "  [ 0.99165899 -0.92522973  0.97133571 -0.80124253  0.58539099]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999917 -0.99999917  0.88607311  0.7366519   0.4490065 ]\n",
      " [ 0.99804538 -0.99902999  0.28130701  0.68677801  0.68291724]\n",
      " [ 0.99971676 -0.99965465  0.62960362  0.28283864 -0.40961957]\n",
      " [ 0.99165899 -0.92522973  0.97133571 -0.80124253  0.58539099]]\n"
     ]
    }
   ],
   "source": [
    "print(states_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999917 -0.99999917  0.88607311  0.7366519   0.4490065 ]\n",
      " [ 0.99804538 -0.99902999  0.28130701  0.68677801  0.68291724]\n",
      " [ 0.99971676 -0.99965465  0.62960362  0.28283864 -0.40961957]\n",
      " [ 0.99165899 -0.92522973  0.97133571 -0.80124253  0.58539099]]\n"
     ]
    }
   ],
   "source": [
    "print(last_rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(states_val, last_rnn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test shows that no matter the input, the final state is set by the sequence length as expected. This is important because the word embeddings for key 0 will be entered but the sequence length will not allow the result to be used. Hence one does not need to zero pad but could put in any inputs beyond the sequence length such as 1e6."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

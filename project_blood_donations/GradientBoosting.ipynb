{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile = 'gradient_boosting.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper_methods import load_data\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, y_train, ids_train, features_set = load_data(train=True, engineering=True, standardizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StatifiedCV = StratifiedKFold(y_train, n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter and feature set tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set of features, use stratified cross validation to find the optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "clf = GradientBoostingClassifier()\n",
    "param_grid = dict(learning_rate=[0.01, 0.05, 0.1, 0.2],\n",
    "                  n_estimators=[25, 50, 100, 200],\n",
    "                  subsample=[0.125, 0.25, 0.50, 0.75],\n",
    "                  max_depth=[1, 2, 3])\n",
    "for i, features in enumerate(features_set):\n",
    "    grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='log_loss', n_jobs=-1)\n",
    "    grid.fit(X=df_train[features].values, y=y_train)\n",
    "    scores.append((-grid.best_score_, i, grid.best_params_))\n",
    "    print i, -grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the optimal hyperparameters and feature set; refit the model to the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores.sort()\n",
    "optimal_features = features_set[scores[0][1]]\n",
    "optimal_params = scores[0][2]\n",
    "X_train = df_train[optimal_features].values\n",
    "optimal_clf = clf.set_params(**optimal_params).fit(X_train, y_train)\n",
    "print 'Optimal score: %.2f' % scores[0][0]\n",
    "print 'Optimal features: ', optimal_features\n",
    "print 'Optimal parameters: ', optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data, predict the probabilities and write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test, y_dummy, ids_test, features_set_dummy = load_data(train=False, engineering=True, standardizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = df_test[optimal_features].values\n",
    "y_pred_prob = optimal_clf.predict_proba(X_test)[:,1]\n",
    "df_out = pd.DataFrame({'':ids_test, 'Made Donation in March 2007':y_pred_prob})\n",
    "df_out.to_csv(outfile, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the optimal classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')\n",
    "\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(optimal_clf, X_train, y_train, train_sizes=np.linspace(0.1, 1, 10), cv=10)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='b', marker='o', label='Train accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='b')\n",
    "plt.plot(train_sizes, test_mean, color='g', marker='o', label='Validation accuracy')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n",
    "\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.ylim(0.3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve for the optimal set of parameters and features is shown above. We see that the model is well fit. The shaded region indicate the standard deviation of the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred_cv = cross_val_predict(optimal_clf, X_train, y_train, cv=StatifiedCV)\n",
    "confmat = confusion_matrix(y_true=y_train, y_pred=y_train_pred_cv)\n",
    "print confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function cross_val_predict has a similar interface to cross_val_score, but returns for each element in the input the prediction that was obtained for that element when it was in the test set. It does not offer a predict_proba method yet but this is under discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print 'Accuracy:  %.2f' % accuracy_score(y_true=y_train, y_pred=y_train_pred_cv)\n",
    "print 'Recall:    %.2f' % recall_score(y_true=y_train, y_pred=y_train_pred_cv)\n",
    "print 'Precision: %.2f' % precision_score(y_true=y_train, y_pred=y_train_pred_cv)\n",
    "print 'F1 score:  %.2f' % f1_score(y_true=y_train, y_pred=y_train_pred_cv)\n",
    "print 'Log loss:  %.2f' % log_loss(y_true=y_train, y_pred=y_train_pred_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test if a classification score is significant, a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import permutation_test_score\n",
    "\n",
    "score, permutation_scores, pvalue = permutation_test_score(optimal_clf, X_train, y_train, cv=StatifiedCV, n_permutations=100, scoring='accuracy')\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores')\n",
    "plt.plot(2 * [score], plt.ylim(), '--g', linewidth=2, label='Classification score (pvalue {:.2f})'.format(pvalue))\n",
    "plt.plot(2 * [1. / 2], plt.ylim(), '--r', linewidth=2, label='Random guessing')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0.45, 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Monday, March 27, 2017\n",
    "# Part 13: Simple predictive models including Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('halverson')\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iofile = 'data/fightmetric_cards/fightmetric_fights_CLEAN_3-6-2017.csv'\n",
    "fights = pd.read_csv(iofile, header=0, parse_dates=['Date'])\n",
    "fights.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove no contests immediately to avoid inf (infinity) in WinRatio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fights = fights[fights.Outcome != 'no contest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = []\n",
    "num_wins = []\n",
    "num_L = []\n",
    "num_wins_L = []\n",
    "for index, row in fights.iterrows():\n",
    "     d = row['Date']\n",
    "     \n",
    "     winner = row['Winner']\n",
    "     x = fights[((fights.Winner == winner) | (fights.Loser == winner)) & (fights.Date < d) & (fights.Outcome != 'no contest')].shape[0]\n",
    "     y = fights[(fights.Winner == winner) & (fights.Date < d)].shape[0]\n",
    "     num.append(x)\n",
    "     num_wins.append(y)\n",
    "     \n",
    "     loser = row['Loser']\n",
    "     x = fights[((fights.Winner == loser) | (fights.Loser == loser)) & (fights.Date < d) & (fights.Outcome != 'no contest')].shape[0]\n",
    "     y = fights[(fights.Winner == loser) & (fights.Date < d)].shape[0]\n",
    "     num_L.append(x)\n",
    "     num_wins_L.append(y)\n",
    "fights['Num'] = num\n",
    "fights['Num_L'] = num_L\n",
    "fights['Num_Wins'] = num_wins\n",
    "fights['Num_Wins_L'] = num_wins_L\n",
    "fights['WinRatio'] = fights.Num_Wins / fights.Num\n",
    "fights['WinRatio_L'] = fights.Num_Wins_L / fights.Num_L\n",
    "len(num), len(num_L), max(num), max(num_L), min(num), min(num_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 'Gray Maynard'\n",
    "fights[(fights.Winner == x) & (fights.Winner == x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fights[fights.WinRatio_L > 1][['Winner', 'WinRatio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that win ratio can be NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fights.WinRatio.loc[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iofile = 'data/ufc_name_education.csv'\n",
    "ufc = pd.read_csv(iofile, header=0)\n",
    "ufc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iofile = 'data/fightmetric_fighters_with_corrections_from_UFC_Wikipedia_CLEAN.csv'\n",
    "fighters = pd.read_csv(iofile, header=0, parse_dates=['Dob'])\n",
    "cols = ['Name', 'Height', 'Reach', 'LegReach', 'Stance', 'Dob']\n",
    "df = fights.merge(fighters[cols], how='left', left_on='Winner', right_on='Name')\n",
    "df = df.merge(fighters[cols], how='left', left_on='Loser', right_on='Name', suffixes=('', '_L'))\n",
    "df = df.drop(['Name', 'Name_L'], axis=1)\n",
    "df = df.merge(ufc, left_on='Winner', right_on='Name', how='left')\n",
    "df = df.merge(ufc, left_on='Loser', right_on='Name', how='left', suffixes=('', '_L'))\n",
    "df.Education = df.Education.fillna(0.0)\n",
    "df.Education_L = df.Education_L.fillna(0.0)\n",
    "df = df.drop(['Name', 'Name_L'], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iofile = 'data/fightmetric_career_stats.csv'\n",
    "cstats = pd.read_csv(iofile, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(cstats, left_on='Winner', right_on='Name', how='left')\n",
    "df = df.merge(cstats, left_on='Loser', right_on='Name', how='left', suffixes=('', '_L'))\n",
    "df = df.drop(['Name', 'Name_L'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter fights to defeats since January 1, 2005:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs = df[(df.Date > pd.to_datetime('2005-01-01')) & (df.Outcome == 'def.')].copy()\n",
    "fs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Younger wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yw = fs[pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L) & (fs.Dob != fs.Dob_L)]\n",
    "wins = yw[yw.Dob > yw.Dob_L].shape[0]\n",
    "total = yw.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer reach wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = fs[pd.notnull(fs.Reach) & pd.notnull(fs.Reach_L) & (fs.Reach != fs.Reach_L)]\n",
    "wins = lr[lr.Reach > lr.Reach_L].shape[0]\n",
    "total = lr.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't exclude nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = fs[fs.Reach != fs.Reach_L]\n",
    "wins = lr[lr.Reach > lr.Reach_L].shape[0]\n",
    "total = lr.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "71.0 > np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "71.0 == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr[['Reach', 'Reach_L']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that nulls in Reach_L lead to Falses which increased the total which gave a win ratio less than 1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taller wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ht = fs[pd.notnull(fs.Height) & pd.notnull(fs.Height_L) & (fs.Height != fs.Height_L)]\n",
    "wins = ht[ht.Height > ht.Height_L].shape[0]\n",
    "total = ht.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = fs[pd.notnull(fs.Stance) & pd.notnull(fs.Stance_L) & (((fs.Stance == 'Orthodox') & (fs.Stance_L == 'Southpaw')) | ((fs.Stance == 'Southpaw') & (fs.Stance_L == 'Orthodox')))]\n",
    "wins = st[st.Stance == 'Southpaw'].shape[0]\n",
    "total = st.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed = fs[pd.notnull(fs.Education) & pd.notnull(fs.Education_L) & (fs.Education != fs.Education_L)]\n",
    "wins = ed[ed.Education == 1].shape[0]\n",
    "total = ed.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leg reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lg = fs[pd.notnull(fs.LegReach) & pd.notnull(fs.LegReach_L) & (fs.LegReach != fs.LegReach_L)]\n",
    "wins = lg[lg.LegReach > lg.LegReach_L].shape[0]\n",
    "total = lg.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict winner has more UFC fights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nf = fs[pd.notnull(fs.Num) & pd.notnull(fs.Num_L) & (fs.Num != fs.Num_L)]\n",
    "wins = nf[nf.Num > nf.Num_L].shape[0]\n",
    "total = nf.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict winner has higher win ratio (for 5 fights or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wr = fs[pd.notnull(fs.WinRatio) & pd.notnull(fs.WinRatio_L) & (fs.WinRatio != fs.WinRatio_L) & (fs.Num > 4) & (fs.Num_L > 4)]\n",
    "wins = wr[wr.WinRatio > wr.WinRatio_L].shape[0]\n",
    "total = wr.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How does the result change if we replace NaN with 0.5 and don't require a certain number of fights?\n",
    "# Ans: (1571, 2728, 0.5758797653958945, 833, 2.331962646952389e-15)\n",
    "#fs.WinRatio = fs.WinRatio.fillna(0.5)\n",
    "#fs.WinRatio_L = fs.WinRatio_L.fillna(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher win ratio and younger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.WinRatio) & pd.notnull(fs.WinRatio_L) & pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L) & (fs.WinRatio != fs.WinRatio_L) & (fs.Dob != fs.Dob_L) & (fs.Num > 4) & (fs.Num_L > 4)]\n",
    "msk1 = (vd.WinRatio > vd.WinRatio_L) & (vd.Dob > vd.Dob_L)\n",
    "msk2 = (vd.WinRatio < vd.WinRatio_L) & (vd.Dob < vd.Dob_L)\n",
    "\n",
    "sp = vd[msk1 | msk2]\n",
    "wins = sp[sp.WinRatio > sp.WinRatio_L].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reach advantage of 4 inches or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.Reach) & pd.notnull(fs.Reach_L)]\n",
    "msk1 = np.abs(vd.Reach - vd.Reach_L) >= 4\n",
    "\n",
    "sp = vd[msk1]\n",
    "wins = sp[(sp.Reach > sp.Reach_L)].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp[['Winner', 'Reach', 'Loser', 'Reach_L']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 years or more younger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L)]\n",
    "msk1 = ((vd.Dob - vd.Dob_L) / np.timedelta64(1, 'Y'))**2 >= 16\n",
    "\n",
    "sp = vd[msk1]\n",
    "wins = sp[(sp.Dob > sp.Dob_L)].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp[['Winner', 'Dob', 'Loser', 'Dob_L']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Younger and greater reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L) & pd.notnull(fs.Reach) & pd.notnull(fs.Reach_L)]\n",
    "msk1 = (vd.Reach > vd.Reach_L) & (vd.Dob > vd.Dob_L)\n",
    "msk2 = (vd.Reach < vd.Reach_L) & (vd.Dob < vd.Dob_L)\n",
    "\n",
    "sp = vd[msk1 | msk2]\n",
    "wins = sp[(sp.Dob > sp.Dob_L) & (sp.Reach > sp.Reach_L)].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Southpaw and younger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.Stance) & pd.notnull(fs.Stance_L) & pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L)]\n",
    "msk1 = (vd.Stance == 'Southpaw') & (vd.Stance_L == 'Orthodox') & (vd.Dob > vd.Dob_L)\n",
    "msk2 = (vd.Stance == 'Orthodox') & (vd.Stance_L == 'Southpaw') & (vd.Dob < vd.Dob_L)\n",
    "\n",
    "sp = vd[msk1 | msk2]\n",
    "wins = sp[sp.Stance == 'Southpaw'].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South, younger and greater reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vd = fs[pd.notnull(fs.Stance) & pd.notnull(fs.Stance_L) & pd.notnull(fs.Dob) & pd.notnull(fs.Dob_L) & pd.notnull(fs.Reach) & pd.notnull(fs.Reach_L)]\n",
    "msk1 = (vd.Stance == 'Southpaw') & (vd.Stance_L == 'Orthodox') & (vd.Reach > vd.Reach_L) & (vd.Dob > vd.Dob_L)\n",
    "msk2 = (vd.Stance == 'Orthodox') & (vd.Stance_L == 'Southpaw') & (vd.Reach < vd.Reach_L) & (vd.Dob < vd.Dob_L)\n",
    "\n",
    "sp = vd[msk1 | msk2]\n",
    "wins = sp[sp.Stance == 'Southpaw'].shape[0]\n",
    "total = sp.shape[0]\n",
    "wins, total, wins / float(total), fs.shape[0] - total, 2 * binom.cdf(p=0.5, k=min(wins, total - wins), n=total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp[['Winner', 'Dob', 'Reach', 'Stance', 'Loser', 'Dob_L', 'Reach_L', 'Stance_L']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall win ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_fighters = fs.Winner.append(fs.Loser).value_counts().to_frame()\n",
    "wins = fs.Winner.value_counts().to_frame()\n",
    "loses = fs.Loser.value_counts().to_frame()\n",
    "fs_fighters = fs_fighters.merge(wins, left_index=True, right_index=True, how='left')\n",
    "fs_fighters = fs_fighters.merge(loses, left_index=True, right_index=True, how='left')\n",
    "fs_fighters = fs_fighters.fillna(0.0)\n",
    "fs_fighters.columns = ['Total', 'Wins', 'Losses']\n",
    "fs_fighters['WinRatio'] = fs_fighters.Wins / (fs_fighters.Wins + fs_fighters.Losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws = fs.merge(fs_fighters[['Total', 'WinRatio']], left_on='Winner', right_index=True, how='left')\n",
    "ws = ws.merge(fs_fighters[['Total', 'WinRatio']], left_on='Loser', right_index=True, how='left', suffixes=('', '_L'))\n",
    "ws.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws = ws[pd.notnull(ws.WinRatio) & pd.notnull(ws.WinRatio_L) & (ws.Total > 5) & (ws.Total_L > 5)]\n",
    "wins = ws[ws.WinRatio > ws.WinRatio_L].shape[0]\n",
    "total = ws.shape[0]\n",
    "wins, total, wins / float(total), ws.shape[0] - total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Cleaning, imputing and standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs['Age'] = (fs.Date - fs.Dob) / np.timedelta64(1, 'Y')\n",
    "fs['Age_L'] = (fs.Date - fs.Dob_L) / np.timedelta64(1, 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs.Stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs.Stance_L.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = fs[fs.Stance.isin(['Southpaw', 'Orthodox', 'Switch']) & fs.Stance_L.isin(['Southpaw', 'Orthodox', 'Switch'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NaN's in win ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.WinRatio = gs.WinRatio.fillna(0.5)\n",
    "gs.WinRatio_L = gs.WinRatio_L.fillna(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.Stance = gs.Stance.replace({'Switch':'Orthodox'}).replace({'Southpaw':1, 'Orthodox':0})\n",
    "gs.Stance_L = gs.Stance_L.replace({'Switch':'Orthodox'}).replace({'Switch':2, 'Southpaw':1, 'Orthodox':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gs.LegReach = gs.LegReach.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ['Reach', 'Height', 'LegReach', 'Stance', 'Num', 'WinRatio', 'Education', 'Age', 'Reach_L', 'Height_L', 'LegReach_L', 'Stance_L', 'Num_L', 'WinRatio_L', 'Education_L', 'Age_L']\n",
    "cols = ['Reach', 'Height', 'Stance', 'Num', 'WinRatio', 'Education', 'Age', 'Reach_L', 'Height_L', 'Stance_L', 'Num_L', 'WinRatio_L', 'Education_L', 'Age_L']\n",
    "gs[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     slpm           4026 non-null float64\n",
    "     str_acc        4026 non-null float64\n",
    "     sapm           4026 non-null float64\n",
    "     str_def        4026 non-null float64\n",
    "     td_avg         4026 non-null float64\n",
    "     td_acc         4026 non-null float64\n",
    "     td_def         4026 non-null float64\n",
    "     sub_avg        4026 non-null float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gs = gs[cols]\n",
    "gs['AgeDiff'] = gs.Age - gs.Age_L\n",
    "gs['ReachDiff'] = gs.Reach - gs.Reach_L\n",
    "gs['StanceDiff'] = gs.Stance - gs.Stance_L\n",
    "gs['EducationDiff'] = gs.Education - gs.Education_L\n",
    "gs['WinRatioDiff'] = gs.WinRatio - gs.WinRatio_L\n",
    "gs['NumFightsDiff'] = gs.Num - gs.Num_L\n",
    "gs['SApMDiff'] = gs.sapm - gs.sapm_L\n",
    "gs['SLpMDiff'] = gs.slpm - gs.slpm_L\n",
    "gs['TDavgDiff'] = gs.td_avg - gs.td_avg_L\n",
    "gs['TDdefDiff'] = gs.td_def - gs.td_def_L\n",
    "gs['SubavgDiff'] = gs.sub_avg - gs.sub_avg_L\n",
    "gs = gs[['AgeDiff', 'ReachDiff', 'StanceDiff', 'EducationDiff', 'WinRatioDiff', 'NumFightsDiff', 'SApMDiff', 'SLpMDiff', 'TDavgDiff', 'TDdefDiff', 'SubavgDiff']]\n",
    "gs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled = []\n",
    "from random import random as rng\n",
    "for index, row in gs.iterrows():\n",
    "     if (rng() < 0.5):\n",
    "          #shuffled.append(([1] + list(row[7:].values) + list(row[:7].values)))\n",
    "          shuffled.append(([1] + list(-1 * row.values)))\n",
    "     else:\n",
    "          shuffled.append(([0] + list(row.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(shuffled)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [0, 1, 3, 7, 8, 10, 14]\n",
    "cols = [0, 7, 14]\n",
    "#cols = [0, 3, 10]\n",
    "cols = range(data.shape[1])\n",
    "data = data.loc[:,cols].dropna()\n",
    "data = data.sample(data.shape[0], replace=False, axis=0)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(data[data.loc[:,0]==0].iloc[:,1], data[data.loc[:,0]==0].iloc[:,2], marker='+')\n",
    "plt.scatter(data[data.loc[:,0]==1].iloc[:,1], data[data.loc[:,0]==1].iloc[:,2], marker='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = dict(max_depth=range(1, 10), criterion=['gini', 'entropy'], splitter=['best', 'random'])\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     clf = DecisionTreeClassifier(random_state=0)\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the optimal tree to graph file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://webgraphviz.com (paste contents of file here)\n",
    "#from sklearn import tree\n",
    "#fn = ['stance1', 'age1', 'stance2', 'age2']\n",
    "#cn = ['fighter1 wins', 'fighter2 wins']\n",
    "#clf.set_params(**grid.best_params_).fit(X_train, y_train)\n",
    "#tree.export_graphviz(clf, out_file='tree.dot', feature_names=fn, class_names=cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment below to see log\n",
    "#grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confmat = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "     for j in range(confmat.shape[1]):\n",
    "          ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.recall_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 \n",
    "y_pred_prob = grid.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('False positive rate (1 - specificity)')\n",
    "plt.ylabel('True positive rate (sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = dict(criterion=['gini', 'entropy'], bootstrap=[True, False])\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     clf = RandomForestClassifier(n_estimators=100)\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = dict(C=np.logspace(-3, 1, base=10, num=20), penalty=['l1', 'l2'])\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     clf = LogisticRegression()\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = dict(learning_rate=np.logspace(-4, 0, base=10, num=5),\n",
    "                  n_estimators=[25, 50, 100],\n",
    "                  base_estimator__max_depth=[1, 2, 3])\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     tree = DecisionTreeClassifier(criterion='entropy')\n",
    "     clf = AdaBoostClassifier(tree)\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = [dict(kernel=['linear'],\n",
    "                   C=np.logspace(-3, 3, base=10, num=15),\n",
    "                   shrinking=[True, False])]\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     clf = SVC(probability=True)\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "stdsc = StandardScaler()\n",
    "StatifiedCV = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = dict(n_estimators=[25, 50],\n",
    "                  bootstrap_features=[True, False],\n",
    "                  base_estimator__C=np.logspace(-3, 3, base=10, num=15),\n",
    "                  base_estimator__penalty=['l1', 'l2'])\n",
    "test_accuracies = []\n",
    "for i in range(10):\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "     lr = LogisticRegression()\n",
    "     clf = BaggingClassifier(base_estimator=lr)\n",
    "     grid = GridSearchCV(clf, param_grid, cv=StatifiedCV, scoring='accuracy', refit=True)\n",
    "     grid.fit(stdsc.fit_transform(X_train), y_train)\n",
    "     # predict test labels\n",
    "     y_test_pred = grid.predict(stdsc.transform(X_test))\n",
    "     test_accuracy = metrics.accuracy_score(y_test_pred, y_test)\n",
    "     test_accuracies.append(test_accuracy)\n",
    "     print i, grid.best_score_, grid.best_params_, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(test_accuracies).mean()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

\clearpage
\section{Predicting the Winner}

In this section we create several models to try to predict
the winner. We start we simple models based on rules
and move on to machine learning models such as
random forest, support vector machine and extreme
gradient boosting.

\subsection*{Simple Rules}

If we exclude no contests and draws, since January 1, 2005,
there have been 3561 UFC fights with a winner and loser.
Table~\ref{table_simple_rules} shows the accuracy score
for several simple rule-based models. For instance, if
our model is that the younger fighter always wins, we would
be correct 1964 times out of 3529 fights or 55.7\%. Note that
there are 32 excluded fights where either the dates of birth
are unknown or both fighters have the same date of birth.

\begin{center}
\begin{table}[h]
\begin{tabular}{r|ccccc}
  \toprule
  Model & Accuracy & Wins & Total & Excluded & $\log_{10}p$\\
  \hline
  Lesser age wins & 55.7\% & 1964 & 3529 & 32 & $-11$\\
  Lesser age by 4 years or more wins & 59.1\% & 893 & 1511 & 2050 & $-12$\\
  Greater reach wins & 52.4\% & 1499 & 2863 & 698 & $-2$\\
  Greater reach by 4 inches or more wins & 55.5\% & 523 & 942 & 2619 & $-4$\\
  Greater height wins & 51.2\% & 1486 & 2903 & 658 & $-0.7$\\
  Greater leg reach wins & 49.0\% & 487 & 993 & 2568 & $-0.2$\\
  Southpaw (vs. orthodox) wins & 55.5\% & 561 & 1010 & 2551 & $-4$\\
  Education (vs. no education) wins & 56.5\% & 594 & 1051 & 2510 & $-5$\\
  Greater number of UFC fights wins & 55.6\% & 1606 & 2886 & 675 & $-9$\\
  Fighter who fought more recently wins & 51.8\% & 1220 & 2353 & 1208 & $-1.1$\\
  Greater previous win ratio wins & 58.6\% & 434 & 740 & 2821 & $-6$\\
  Current or former champion wins & 61.8\% & 175 & 283 & 3278 & $-5$\\
  \hline
  Lesser age and greater reach wins & 57.2\% & 919 & 1607 & 1954 & $-9$\\
  Southpaw and lesser age wins & 61.7\% & 284 & 460 & 3101 & $-7$\\
  Southpaw, lesser age and greater reach wins & 62.0\% & 132 & 213 & 3348 & $-4$\\
  Greater previous win ratio and lesser age wins & 61.9\% & 252 & 407 & 3154 & $-6$\\
  \bottomrule
\end{tabular}
\caption{Accuracy scores for simple rule-based models. For models where win ratio was used, we only considered
fighters with 5 or more UFC fights. $p$-values are given in the rightmost column. Note that for $p=0.05$, $\log_{10}p = -1.3$. In all
cases the sum of 
Total and Excluded is 3561.}
\label{table_simple_rules}
\end{table}
\end{center}

In computing the number of previous UFC fights for each fighter, we start
counting with UFC 1. We ignored no contests but draws were included. A fighter's
win ratio is their number of UFC wins divided by their
total number of UFC fights (including draws).

\subsection*{Machine Learning Models}

For each fighter we have the following features:

\begin{itemize}[noitemsep]
  \item Height
  \item Reach
  \item Leg reach
  \item Stance (orthodox, southpaw or switch)
  \item Age
  \item Education (yes or no)
  \item Current or former champion (yes or no)
  \item Number of previous UFC fights
  \item Total time in the Octogon
  \item Elapsed time since last UFC fight
  \item Win percentage of UFC fights
  \item Percentage of wins by KO/TKO
  \item Percentage of wins by submission
  \item Percentage of wins by decision
  \item Percentage of losses by KO/TKO
  \item Percentage of losses by submission
\end{itemize}

\noindent
We also use the following career statistics:

\begin{itemize}[noitemsep]
  \item Significant strikes landed per minute
  \item Significant striking accuracy
  \item Significant strikes absorbed per minute
  \item Significant strike defense
  \item Average takedowns landed per 15 minutes
  \item Takedown accuracy
  \item Takedown defense
  \item Average submissions attempted per 15 minutes
\end{itemize}

\noindent
The derived features are

\begin{itemize}[noitemsep]
  \item Difference in age, reach, number of fights, and so on of the two fighters
  \item Total damage = total fight time times strikes absorbed per minute
  \item Total strikes thrown per minute = SlPM / striking accuracy
  \item Natural logarithm of elapsed time since last UFC fight
  \item Fighter versatility index = (3/2)((1/3)/(W2+W2+W2) - 1/3)
\end{itemize}

\noindent
These features depend on both fighters:

\begin{itemize}[noitemsep]
  \item Expected strikes absorbed per minute1 = string defense1 * (SlPM / striking accuracy)2
  \item TKO susceptibility $(\textrm{LTKO}_1 \times \textrm{WTKO}_2)^{1/2}$
  \item Submission susceptibility $(\textrm{LSUB}_1 \times \textrm{WSUB}_2)^{1/2}$
\end{itemize}

The class label is a 0 if fighter 1 beats fighter 2 or
a 1 if fighter 2 beats fighter 1.
There are approximatley
equal numbers of each class label.

As always we need discriminatory features.

An additional feature would be an indicator variable
which would be unity for champions and interim champions
and zero otherwise. The idea here is that champions
impose fear in their opponents which gives them a slight
advantage. Even former champions.

Another idea is to rate wins and losses. Losing to a good
opponent should hurt you less and beating a good fighter
should get you more. Hence the win ratio should be
weighted. In this study the ratio was unweighted.

Fighters who are susceptible to TKO or submission
losses and fighting an opponent who is particularly
good at either of their should enter.

In many ways the anthropomorphic features are taken
into account by the other metrics and vice versa.
You can't have a great win ratio without being
young and have reach?

The idea is make columns of differences between
quantities. For instance, the difference in
height is $\Delta h = h_1 - h_2$. One could also leave
in the height values of 1 or 2 but we assume this has
little discriminatory power.

\subsection*{Prediction}

One could impute missing data. For instance, leg reach
could be inferred by using height and reach data for
each weight class. For simplicity we ignore fights
where data is missing. This includes cases where one
of the fights did not use the orthodox or southpaw
stances. We are left with 2940 fights.

It is important to note that match makers try to make
the fights fair. This is to prevent fighters from taking
a beating and to make sure they are at the right level.

The idea is to randomly choose 70\% of the fights and
use those to train a model using supervised
learning techniques.
Training a model is a mathematical optimization problem.
There are several different models. Some 
In many cases a function with a given form is presented
and the goal is to find the set of coefficients or weights
which minimize a prescribed error function. In the present
case we seek to optimize the accuracy.
Once the model has been
trained we use it to make fight predictions on
the remaining 30\% of the fights. We then report the
accuracy of the model on the test data.

\begin{center}
\begin{table}[h]
\begin{tabular}{c|cccccc}
  \toprule
  {} &  \multicolumn{2}{c}{{Fighter 1}} & \multicolumn{2}{c}{{Fighter 2}} & Fighter 1 $-$ Fighter 2 & Fighter 1 / Fighter 2\\
  \hline
  Outcome & Age & Reach & Age & Reach & $\textrm{SLpM}_1 - \textrm{SLpM}_2$ & $\textrm{WTKO}_1/\textrm{LTKO}_2$\\
  \hline
  1 & 23.2 & 72.0 & 31.0 & 70.0 & 1.1 & 3.1\\
  0 & 33.5 & 67.0 & 29.9 & 68.0 & $-0.7$ & 0.9\\
  1 & 27.1 & 78.0 & 36.3 & 74.0 & 0.9 & 1.8\\
  \bottomrule
\end{tabular}
\caption{Accuracy scores for simple rule-based models. For models where win ratio was used, we only considered
fighters with 5 or more UFC fights. $p$-values are given in the rightmost column. Note that for $p=0.05$, $\log_{10}p = -1.3$. In all
cases the sum of
Total and Excluded is 3561.}
\label{sample_labels_features}
\end{table}
\end{center}


We create predictive models using the 3561 fights since
January 1, 2005. The idea is to split the data into
training and test sets. We use a 70/30 split. We then
use stratified K-fold cross validation with 10 folds
to find the optimal
hyperparameters for a given model. The model is then
used to make predictions about the test data. The accuracy
is computed.

For non-tree based methods, it is necessary to standardize
each column. The standardizer is applied in preprocessing to
the test data.


\begin{center}
\begin{table}[h]
\begin{tabular}{r|ccccc}
  \toprule
  Model & Features & Accuracy\\
  \hline
  Random forest & $h_1$, $r_1$, $h_2$, $r_2$ & 52.9\% \\
  Logistic regression & $h_1$, $r_1$, $h_2$, $r_2$ & 52.1\% \\
  Random forest & $\Delta h_{12}$, $\Delta r_{12}$ & 51.6\% \\
  Logistic regression & $\Delta h_{12}$, $\Delta r_{12}$ & 51.6\% \\
  Random forest & $h_1$, $\Delta h_{12}$, $r_1$, $\Delta r_{12}$ & 52.0\% \\
  Logistic regression & $h_1$, $\Delta h_{12}$, $r_1$, $\Delta r_{12}$ & 51.9\% \\
  Random forest & $h_1$, $r_1$, $l_1$, $h_2$, $r_2$, $l_2$ & 56.6\% \\
  Logistic regression & $h_1$, $r_1$, $l_1$, $h_2$, $r_2$, $l_2$ & 51.9\% \\
  Random forest & $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$ & 50.7\% \\
  Logistic regression & $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$ & 52.2\% \\
  Random forest & $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$ & 52.4\% \\
  LR & $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$ & 55.1\% \\
  MLP & $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$ & 55.4\% \\
  Random forest & $h_1$, $r_1$, $a_1$, $h_2$, $r_2$, $a_2$ & 53.4\% \\
  Logistic regression & $h_1$, $r_1$, $a_1$, $h_2$, $r_2$, $a_2$ & 54.7\% \\
  Random forest & $h_1$, $r_1$, $a_1$, $a_1^2$, $h_2$, $r_2$, $a_2$, $a_2^2$ & 53.4\% \\
  LR & $h_1$, $r_1$, $a_1$, $a_1^2$, $h_2$, $r_2$, $a_2$, $a_2^2$ & 55.1\% \\
  Random forest & $r_1/h_1$, $a_1^2$, $r_2/h_2$, $a_2^2$ & 54.3\%\\
  LR & $r_1/h_1$, $a_1^2$, $r_2/h_2$, $a_2^2$ & 55.1\%\\
  Multilayer Perceptron & $r_1/h_1$, $a_1^2$, $r_2/h_2$, $a_2^2$ & 55.2\%\\
  Random forest & $h_1$, $r_1$, $l_1$, $a_1$, $h_2$, $r_2$, $l_2$, $a_2$ & 56.1\% \\
  Logistic regression & $h_1$, $r_1$, $l_1$, $a_1$, $h_2$, $r_2$, $l_2$, $a_2$ & 54.9\% \\
  Random forest & $h_1$, $r_1$, $l_1$, $a_1$, $h_2$, $r_2$, $l_2$, $a_2$, $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$ & 55.8\% \\
  Logistic regression & $h_1$, $r_1$, $l_1$, $a_1$, $h_2$, $r_2$, $l_2$, $a_2$, $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$ & 54.9\% \\
  Random forest & $a_1$, $osw_1$, $a_2$, $osw_2$ & 51.0\% \\
  Logistic regression & $a_1$, $osw_1$, $a_2$, $osw_2$ & 55.0\% \\
  Random forest & $a_1$, $osw_1$, $r_1$, $a_2$, $osw_2$, $r_2$ & 53.5\%\\
  Logistic regression & $a_1$, $osw_1$, $r_1$, $a_2$, $osw_2$, $r_2$ & 55.7\%\\
  Random forest & $a_1$, $osw_1$, $r_1$, $l_1$, $a_2$, $osw_2$, $r_2$, $l_2$ & 56.1\%\\
  Logistic regression & $a_1$, $osw_1$, $r_1$, $l_1$, $a_2$, $osw_2$, $r_2$, $l_2$ & 55.4\%\\
  Random forest & $a_1$, $osw_1$, $h_1$, $r_1$, $l_1$, $a_2$, $osw_2$, $h_2$, $r_2$, $l_2$ & 56.7\%\\
  Logistic regression & $a_1$, $osw_1$, $h_1$, $r_1$, $l_1$, $a_2$, $osw_2$, $h_2$, $r_2$, $l_2$ & 55.3\%\\
  Random forest & $h_1$, $r_1$, $c_1$, $h_2$, $r_2$, $c_2$ & 53.3\% \\
  Logistic regression & $h_1$, $r_1$, $c_1$, $h_2$, $r_2$, $c_2$ & 52.8\% \\
  Random forest & $h_1$, $r_1$, $c_1$, $e_1$, $h_2$, $r_2$, $c_2$, $e_2$ & 52.7\% \\
  Logistic regression & $h_1$, $r_1$, $c_1$, $e_1$, $h_2$, $r_2$, $c_2$, $e_2$ & 53.4\% \\
  \hline
  Random forest & $h_1$, $r_1$, $l_1$, $a_1$, $osw_1$, $h_2$, $r_2$, $l_2$, $a_2$, $osw_2$, $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$, $\Delta osw_{12}$ & 56.0\% \\
  LR & $h_1$, $r_1$, $l_1$, $a_1$, $osw_1$, $h_2$, $r_2$, $l_2$, $a_2$, $osw_2$, $\Delta h_{12}$, $\Delta r_{12}$, $\Delta l_{12}$, $\Delta a_{12}$, $\Delta osw_{12}$ & 54.8\% \\
  \bottomrule
\end{tabular}
\caption{}
\label{anthropomorphic_features}
\end{table}
\end{center}



\begin{center}
\begin{table}[h]
\begin{tabular}{r|ccccc}
\toprule
Model & Features & Accuracy\\
\hline
RF & $n_f$, $n_w$, $a$, $t_{total}$ & 53.5\% \\
LR & $n_f$, $n_w$, $a$, $t_{total}$ & 56.9\% \\
RF & $n_f$, $n_w$, $a$, $r$ & 55.1\% \\
LR & $n_f$, $n_w$, $a$, $r$ & 57.0\% \\
RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $c$ & 57.8\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $c$ & 58.0\% \\

RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$ & 57.2\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$ & 58.8\% \\
AdaBoost & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$ & 58.1\% \\

RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$ & 58.0\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$ & 58.8\% \\

RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $sapm$ & 58.0\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $sapm$ & 58.5\% \\

RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $sapm$, $slpm$ & 57.9\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $sapm$, $slpm$ & 59.1\% \\
MLP & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $sapm$, $slpm$ & 58.9\% \\

RF & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $str_acc$ & 58.6\% \\
LR & $n_f$, $n_w$, $a$, $r$, $h$, $l$, $osw$, $c$, $e$, $str_acc$ & 58.4\% \\

RF & all & 58.1\% \\
LR & all & 59.3\% \\
MLP & all & 59.1\% \\

RF & $n_f$, $n_w$, $a$, $r$, $osw$, $c$ & 56.6\% \\
LR & $n_f$, $n_w$, $a$, $r$, $osw$, $c$ & 58.9\% \\
MLP & $n_f$, $n_w$, $a$, $r$, $osw$, $c$ & 58.5\% \\
RF & $n_f$, $n_w$, $a$, $r$, $osw$, $c$, $h$, $l$ & 57.9\% \\
LR & $n_f$, $n_w$, $a$, $r$, $osw$, $c$, $h$, $l$ & 59.0\% \\
RF & $a$, $r$, $slpm$ & 54.9\% \\
LR & $a$, $r$, $slpm$ & 56.2\% \\
RF & $a$, $r$, $takedowns$ & 54.3\% \\
LR & $a$, $r$, $takedowns$ & 56.1\% \\
RF & $a$, $r$, $sapm$ & 55.7\% \\
LR & $a$, $r$, $sapm$ & 57.1\% \\
\bottomrule
\end{tabular}
\caption{}
\label{other_features}
\end{table}
\end{center}



\begin{center}
\begin{table}[h]
\begin{tabular}{r|ccccc}
  \toprule
  Model & Accuracy & Notes\\
  \hline
  Decision trees & 60.7\% & Varied depth of tree and splitting criterion\\
  Random forest & 63.5\% & Varied number of estimators and splitting criterion\\
  Logistic regression & 65.6\% & Varied regularization parameter and L1/L2\\
  Adaptive Boosting & 55.7\% & Varied learning rate\\
  Support Vector Classifier & \% & Varied linear and nonlinear kernels\\
  Bagging with Logistic Regression & \% & Varied number of estimators and regularization\\
  \bottomrule
\end{tabular}
\caption{Accuracy scores for simple rule-based models. For models where win ratio was used, we only considered
fighters with 5 or more UFC fights. $p$-values are given in the rightmost column. Note that for $p=0.05$, $\log_{10}p = -1.3$. In all
cases the sum of
Total and Excluded is 3561.}
\label{table_ML_models}
\end{table}
\end{center}

The accuracy scores of various predictive models are found in Table~\ref{table_ML_models}.
We see the best model was found to be random forest with an accuracy of 56\%. This is
surprising given that the rule-based models in Table~\ref{table_simple_rules} had a
similar predictive power even though they were limited to a subset of the fights.
In the end the prediction is difficult because matchmakers try to pair fighters with
roughly equal ability.

\begin{center}
\begin{table}[h]
\input{prediction/single_feature_logistic.tex}
\caption{Accuracy and Brier scores for single features using logistic regression.}
\end{table}
\end{center}

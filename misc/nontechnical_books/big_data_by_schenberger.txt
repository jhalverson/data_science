Big Data: A revolution that will transform how we live, work and think
Viktor Mayer-Schonberger and Kenneth Cukier (2013 version)


== Now ==

Google Flu trends considered half a billion mathematical models. The model
worked faster than the CDC but years later it was shown to fail. The model
was thus once the face of big data but later a model that fell from grace.

1918 Spanish flu killed tens of millions. Google tried to predict the spread
based on search terms from 2003 to 2008 which they data flu data on. Doctors
reporting to the CDC took one or two weeks which was too late. Google's meth-
od didn't require needles or mouth swabs.

Etzioni and FareCast. He asked fellow passengers how much they paid and was
upset to find that buying earlier than anyone he paid among the highest. He
then scraped big data and store the prices as a function of time away from
when the flight was and indicated whether the consumer should buy or wait.
Not a predictive model, just lots of data that had been averaged together.

Farecast was integrated into Microsoft's Bing. Hadoop came out of Yahoo.
Sloan Digital Sky Survey collected more data in its first few weeks than
everything that had been amassed in the entire history of astronomy.

There were 300 exabytes in 2007 with 7 percent analog. In 2013, total information
is 1200 exabytes with only 2 percent non-digital.

Walmart and Capital One pioneered the use of big data in their respective
industries.


== More ==

N=All

The US census is required every 10 years. It used to take more than 10 years
to get the final numbers. Eventually the calculations were done on punch cards.

In 1934 people were still bad at forming representative samples from a
larger population. Using randomness proven to be the best way instead of asking
someone to put together what they felt was a representative sample.

23andMe looks are markers instead of the full sequence. Steve Jobs had his
genome and the genome of his cancer sequenced. This allows treatments to be
more targeted.

Big data comes with granular level resolution of the problem understanding.
All 11 years worth of data was used for the sumo cheating study. The 15th
match of critical importance because only sumos with a 8-7 record or better
are allowed to maintain their professional status. When the match was thrown
the loser tended to win much more so than the winner in subsequent non-15th
matches.

Lytro cameras store lots of information and allow the photographer to decide
what to focus on later.

If a node with few edges (but having some outside their immediate community) is
deleted from a network it has a major effect on keeping the overall notework
together. The vertices with a large number of edges can be removed.

Statistical sampling is less than a century old. Today we can use N=all.


== Messy ==

As we draw from bigger sources, mistakes or inexactitude enter. But these
mistakes are outweighed by the gain of having much more data. An example of
this is a grammar checker of the old days. Without changing the algorithm
but simply adding more more data improve the accuracy dramatically.

In chess, endgames when six or fewer pieces are left have been completely
analyzed and all possible moves are stored. This allows to program to beat
humans because at this point they can literally play flawlessly.

https://translate.google.com
Machine translation started off as a list of rules. Big data changed all that
when huge numbers of translations were produced. Despite the messiness in all
those translated pages (95 billion English sentences).

The consumer price index (CPI) is used to measure inflation. This was done by
having people estimate the price incease in 250k consumer products. This can
now be done by scraping and big data analysis. One such company is pricestats.
One would think that the former decide.com would also be in a position to do
this.

Tagging is a common way to label unstructured data on the Internet like photos
on Flickr or tweets.

The old days was little, perfect data and hypotheses that would be carefully
explored. Today the game is about big messy data and no hypotheses but just go
looking for correlations.

Relational databases worked well in the old days of spare, exact, structured
data that fits on a single machine. In the big data age, NoSQL has proven to
be a necessary shift. These databases support enormous amounts of data which
is distributed over many machines. It affords schemaless data that whose fields
can change dynamically. Hadoop recognizes that the data is so large that it
is best to bypass ETL and just do the analysis where the data is. There was
once Google MapReduce. Only 5 percent of all data is structured.

== Correlation ==

Amazon used to have people write book reviews. In 1997 then started working
towards have a computer make the recommendations which was stated in their
prospectus. Eventually the engine was used and the people were fired. Today
one-third of all Amazon sales are from recommendations. The engine teased out
valuable correlations without knowing the cause or the why.

Walmart has 2 million employees and takes in $450 billion annually. 

Big data allows us to find correlations that were not possible in the small
data world. This means we are given the what but not the why. For instance,
Walmart found that in the days leading up a hurricane, people tend to buy
flashlights and pop tarts.

Credit scores (FICO) are model based. The company also makes predictions for
the healthcare industry. Aviva has a model using hundreds of variables to
predict those at risk o illnesses like high blood pressure, diabetes, or de-
pression.

The Target Story: The company looked at items its customers bought and those
purchased by customer registered in Target's pregnancy club. It them could
predict which customers not the club were pregnant. It sent a flyer of baby
products to one woman who was living with her parent at age 18. The father
complained and later apologized when learning that his teenage daughter was in
fact pregnant.

Because we don't have causation it is important to keep in mind that predictors
or combinations of predictors are proxies to quantities that we are not aware of
or don't have the data of but through correlation we get what we want.

Sensors are being placed all over on bridges, trucks, factory processing equip-
ment. Then use correlation analysis to figure out what is wrong. At a Canadian
University along with IBM, they are recording 16 data streams on pre-mature
babies. They can detect the onset of infection 24 hours before hand. Very constant
vital signs were found before detection.

Network analysis can be used for finding which court decisions are based on
which precedents, who call whom on their cellphones and social friendships
and professional relationships.

A few hundred manholes in NYC get launched in the air a few stories (300 lb.)
because of fires. Con Edison hired a team to come up with a model of ident-
ifying the manholes that are most likely to have problems. The team used
multiple data sets and 106 predictors (initially) to create a model. They
needed to datafy handwritten notes. The top 10% of their predictions contain-
ed 44 percent of the potholes that proved to be severe. The data was messy
but by incorporating many, big data sets, the inexactitutde is outweighed
by the gain of big. Some of their data wasn't machine readable.


== Datafied ==

The story of US Navy member Matthew Fontaine Maury. He is responsible for
data-fying the seas by compiling all records and stories he could acquire and
making sense of it. Ship captains used to follow safe routes that were
circuitous because they were after of crashing or encountering bad weather
by taking routes which were not mapped. He captured the time dependence
of the oceans, ie., the winds and flows are seasonal or the like. He then
started having ships record data to get higher resolution.

Dataficationa and digitization are different processes. A company in Japan
is making seats with 360 sensors in them to measure the persons posture
and to determine who the person is. If a stranger then the car will not start.

Looked at wireless and bluetooth sensors. See the company Monnit.

https://books.google.com/
Google Books: October 2015, the number of scanned book titles was over 25
million but Google thinks there are 130 million distinct books. My PhD thesis
is in the library. The n-gram viewer is described in Uncharted. By adding
books in different languages, one could then use the books for machine
translation. The Gutenberg project also aimed to get books online but they
did not see the secondary values such as n-gram viewer, machine translation
and so on.

GPS satellites are passive in that they send out their position
and time and your phone using triangulation to work out your position.
Geo-location data allows car insurance providers to customize their rates based
on how oftern, how and where a customer drives. UPS uses sensors to optimize
routes and save gas and miles. They have seen formed an external company.

https://genderize.io can be used to estimate the gender of a first name.

FaceBook has datafied friendship. It has 1 billion people and 100 billion
relationships. FaceBook has been slow to sell their data. Twitter sells it data
to two firms: DataSift and Gnip. The twitter firehose is licensed for a variable
amount of money. All tweets are public.

Twitter comes with metadata which includes where the person is, who their
followers are and who they are following. One large scale study published in
Science showed that people's moods followed a similar daily and weekly pattern
across cultures around the world (in 84 countries). Moods have been datafied.

The low hanging fruit of big data has been picked.

One can get a sense of how well a movie will do based on the frequency and
total number of tweets.

https://www.marketpsych.com Analyzes tweets as signals for investments in the
stock market. It teamed up with Thomas Reuters to offer no fewer than 18864
separate indices across 119 countries, on emotional states. This is used not
by humans but by quants.

Quantified self. Apple has a patent on ears buds that measure blood oxygen-
ation, heart rate and body temperature. Wearable technologies like FitBit
and JawBone.

GreenGoose created wireless sensors that could be stick to all kinds of things
like dental floss, cookie jars, and dog collars. Potential uses of this and other
big data cases are only limited by one's imagination.


== Value ==

Captcha and ReCaptcha was invented in the late 1990's. ReCaptcha uses
what people type to figure out scanned text that computers could not
figure out with sufficient certainty. Note: Can log the clicks as well
as mouse movements. Data can be reused unlike food, for example. Big
data tends to have secondard uses. For electric cars, IBM and Honda and
the Pacific Gas and Electric Company identified when the cars should be
refueled and where the refueling stations should be. New uses for data
can be found sometimes many decades later when combined with a new data
set such as the Manhattan pot hole project.

Hitwise measures behavior across desktop, tablet and smartphone devices.
Marketers can use Hitwise to get a sense of whether pink will be in
this spring or black is back.

Amazon was hired to handle the technology to AOL's e-commerce site. Ama-
zon was interested in using their data to improve their own recommendation
engine. AOL did not think about this. Google and Nuance.

A logistic company knews lot about delivering goods and product shipments
so it processed the data and sold economic and business forecasts. Alos,
SWIFT which is the global interbank system that starting offering GDP fore-
casts based on the number and nature of wire transfers.

Recombinant data: Combining data sets to produce value. Do cellphones
increase the likelihood of cancer. The Danish Cancer Society looked at who
had cell phones from 1987 to 1995, a registry of nationwide cancer patients
and education level and disposable income for each inhabitant. These were
excellent data sets of high quality since they did not introduce bias. In fact, 
the data were generated years earlier and for a completely different purpose.
Could use the N=all approach. They were able to control for subpopulations
since they had so much data. Published in 2011.

Data exhaust: The digital trail that people leave in their wake. Spell
checkers can be generated from Internet searches. People are more likely
to leave a FaceBook post if they see that their friend has  done so. This
is a great feature for a classifier that predicts whether or not someone will
post.

Data.gov in the US. FlyOnTime.us appears dead.
The value of FaceBook's data was not contained in its IPO.
Big data can backfire it that it can be used against the company. A gender
equality law suit was able to use the companies data to find out exactly
what the female employees should have gotten if they were paid fairly.


== Implications ==

There is the data, the people who know what to do with it and then the
people how come up with ideas for what to do with big data.

Decide.com was similar to FareCast in that it stored massive amounts of
data on everyday items and then did price prediction. The company had a
"big data mindset". The site has since be bought out by eBay just like
Microsoft bought and then killed FareCast. Very popular is
google.com/flights. Bing now goes through Kayak. Oddly to me, there was
a patent on what FareCast did even though it is straightforward. ITA
was FareCast's data supplier.

McKinsey Global Institute predicts a large shortage of people with the
skills to analyze, store and deal with data.

"A division called MasterCard Advisors aggregates and analyzes 65 billion
transactions from 1.5 billion cardholders in 210 countries ... then sells
the consumer trends. FlightCaster.com predicted flight delays using big
data but was then sold to Next Jump.

Today news articles are recommended to people by algorithms not by editors.
This was first used with Amazon, which is how the book began.

Mobile phone companies could license their data just as Twitter does to two
companies. Udacity is notifying its instructors where students are reviewing
their videos the most so that the instructor can improve them.

All of a cars electronics cost 1/3 the total price of the car. When a car
is serviced this data is downloaded by the dealer and used. One company
found that a fuel tank was faulty and rather than contact the supplier
and telling them, that would then notify their competitors, the company
chose to keep it quiet and solve the problem through software.

Inrix is concerned with using traffic data and selling the insight to other
companies. Inrix gets data from many companies that are competing with one
another but they are comfortable giving their data to Inrix because of
what they gain. Inrix takes the data and returns it to those who supply it.

In 2010 UPS took their extensive traffic monitoring team and created Roadnet
Technologies. Other companies were comfortable handing over their data and
everyone benefitted from the massive data set. The healthcare industry did
a similar thing.

Domain expertise in the small data age loses a lot of ground to someone
with DS skills working with big data. The DS can quickly find correlations
that the expert would have never found.

Video games is a bigger industry than Hollywood films worldwide. Zynga
makes games in a custom way and uses data to know where players are strug-
gling. They learn things about peoples preferences in color and design
patterns which maybe could be applied to the real consumer world.

the-numbers.com gives lots of data on Hollywood movies. Production companies
pay to have their budgets analyzed for the given gendre, subject material
and stars. Predictive models are used to estimate weekend box office results.
The company has 30 million records. The records also have a network nature
where "this screenwriter worked with this director". Producers take the
results to financial backers which gives their case more credence.

Their is an overall study that shows that data-driven companies have
employees that are more productive by 6% in comparison to companies that make
decisions by gut.

Rolls Royce put sensors in their engines and monitor the results for
clients and tell them if something is wrong. They make 70% on their incomes
by doing this. So the engines make less. Obviously, jet engines need
to work and problems need to be anticipated. UPS does this with sensors
since they need reliable trucks.


== Risks ==

During the cold war the Stasi or secret police of East Germany monitor-
ed its citizen in stealth. It was a complex operation with over 100k
people involved, mostly citizens turned informants. The film "The Lives
of Others" was based on this.

Today it is far easier to invade people's lives. The NSA, DHS, CIA, FBI,
local and state police, AFT, Alexa and shopping, combined with FaceBook, Twitter,
Google web searches, Instagram, and cell phone companies and credit card
companies can do orders of magnitude more than the Stasi. There are 30
cameras within 200 yards of where George Orwell wrote 1984.

Furthermore, people can tell what is going on inside a house by looking
at "load signatures" on the electrical grid via electric meters. One can
distinguish between a dryer and a laptop computer.

The Boston PD announced that they would not purchased new software to
scan social media for threats to public safety and the police.

The secondary uses of big data are often unforeseen when the data is
collected. For this reason consent agreements are often unless. For
example, Google Street View also collecting geo-location data for its self-
driving car project.

AOL and NetFlix got burned by releasing big data that was supposed to
be anonymotized. The CTO of AOL was fired in 2006 after it happened.
"60 single men" was the woman's search on AOL. The woman in the Netflix
case a closeted lesbian and was outted by the release of the data.

The NSA intercepts and stores 1.7 billion emails, phone calls and other
communications every day. A 1.2 billion dollar facility in Fort
Williamns Utah has been built to analyze this data. Sign out front reads
"If you have nothing to hide, you have nothing to fear." They are coll-
ecting data that they don't process and storing it for later so that
they can start investigating people immediately. They have the capacity
to store a yottabyte (10^24).

A description of the first scence of Minority
Report (Spielberg) where a man is arrested for a thought crime. The
point is that with model that have been fit by big data, we can have
a good prediction. Such models are used by parole boards in determin-
whether to grant parole or not. A professor has a model to predict
the probability that a parole will be involved in a homocide (the
perp or the victim) with accuracy of 70%. Thus decisions are being made
about what is likely to happen instead of people's actual actions.

With big data profiling can be done a granular level instead of all
people who look like a terrorist.

Humans are hard wired to think that correlation implies causation.

The story of Robert McNamara is that he believed in quantifying
everything before the data-driven approach was popular. He worked for the
US government making significant improvements to armament procurement.
He then worked a Ford applying his numbers-based approach. Things looked
good on the surface at Ford but workers were throwing parts into the near-
by river to satisfy his demands of not building new models until all the
old parts were used. During the Vietnam war when McNamara was the US
Secretary of Defense, success was measured by the
body count. Soldiers and generals lied about the body count by elevating
it. With McNamara taking a numbers approach everything failed. He assumed
progress was being made when it was not. Erroneous data messes up the
whole works and many people paid for it. GIGO.


== Control ==

New institutions and specific jobs are needed combat the perils of big
data since certain groups are vulnerable. "Notice and consent" is an
old approach. Because the uses of big data tend to become clear after
the data is collected, a better approach is to have the users of the
data be responsible for it. There are two clear cases where trying to
anonymotize the data failed: AOL and NetFlix. FaceBook intentionally
blurs the data when releasing big data to such individuals can not be
identified.

Page 177: One concern is punishing people for crimes that they have yet
to commit. Such a practice attacks free will or human volition. Many
models are just black boxes because there are many predictors and models
can be complicated to begin with like neural nets. For instance, in the
case of predicting exploding pot holes in NYC, there were 106 predictors
in the first model. Note: The broad of directors serve the investors not
the management. Monopolies of big data must be avoided with antitrust.

Humans tend to build powerful tools and then figure out how to control
them. Two examples of this are nuclear power and bioengineering. Will
big data prove to be another?

== Next ==

Mike Flowers was appointed director of analytics in Manhattan despite
having a background in law. He was responsible for identifying illegal
conversions which occur when an apartment is divided illegally. Living
in such tight quarters leads to fires, rats and disease, and crime. The
idea was to rank potential conversions that were reported by 311 calls
and tips. Since there were only so many inspectors they couldn't respond
to all the reports. This city project is similar to the potholes, rest-
aurant inspections, led paint removal, bike safety and 311 response time.

Authors seems to at times refer to big data which in fact is not big in
the sense that specialized infrastructure is not required but instead
that many datasets are being used.

For the illegal conversions they used data from 19 different agencies.
They improved the rate of detection from 13 to 70% in effect. Flowers did
not care about causation but only correlation as that solved problems.

The remainder of the book is a summary of what comes before. It points
out the movement of cell phones in African slums to reveal they are
vibrant communities. CERN generates lots of data but only keeps about
0.1% of it.

Main message was (1) N=All is now possible, (2) the downside of inexactness
associated with big data is overcome by all its benefits of scale, and
(3) focus on correlation and give up on causation.

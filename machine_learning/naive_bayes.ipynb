{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Halverson\n",
    "# Tuesday, May 16, 2017\n",
    "# Native Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider writing a Naive Bayes classifier for detecting spam emails versus ham. We wish to compute $P(y|x_1, x_2, ..., x_n)$, where $y$ means the class (spam or ham) and $x_i$ is word $i$ of the vocabulary. According to Bayes' theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y|x_1, x_2, ..., x_n)=\\frac{P(y)P(x_1, x_2, ..., x_n|y)}{P(x_1, x_2, ..., x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make the assumption of independence (this is the naive part):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1, ..., x_{i-1}, x_{i+1}, ..., x_n|y) = P(x_i|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y|x_1, x_2, ..., x_n)=\\frac{P(y)\\prod_i P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator is a constant and the class is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}= arg max(y) P(y)\\prod_i P(x_i|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $P(x_i|y)$ are pre-computed using the training data for each class. The model can then be evaluated for a given test set feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_and_tokenize(person):\n",
    "     # download and parse the biography\n",
    "     base_url = 'https://en.wikipedia.org/wiki/'\n",
    "     r = requests.get(base_url + person)\n",
    "     soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "     # extract the text of each paragraph\n",
    "     raw_text = ''\n",
    "     for paragraph in soup.find_all('p'):\n",
    "          raw_text += paragraph.get_text()\n",
    "\n",
    "     # keep only alphabetical characters and split on whitespace\n",
    "     letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "     words = letters_only.lower().split()\n",
    "\n",
    "     # count the words and filter based on count and stopwords, apply stemming\n",
    "     count = Counter(words)\n",
    "     porter = PorterStemmer()\n",
    "     stops = stopwords.words(\"english\")\n",
    "     words = [porter.stem(word) for word in words if (word not in stops) and (count[word] > 1) and (len(word) > 1)]\n",
    "     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "einstein = scrape_and_tokenize('Albert_Einstein')\n",
    "newton = scrape_and_tokenize('Isaac_Newton')\n",
    "darwin = scrape_and_tokenize('Charles_Darwin')\n",
    "spielberg = scrape_and_tokenize('Steven_Spielberg')\n",
    "allen = scrape_and_tokenize('Woody_Allen')\n",
    "cameron = scrape_and_tokenize('James_Cameron')\n",
    "jordan = scrape_and_tokenize('Michael_Jordan')\n",
    "brady = scrape_and_tokenize('Tom_Brady')\n",
    "williams = scrape_and_tokenize('Serena_Williams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 0, ..., 2, 0, 0],\n",
       "       [0, 0, 3, ..., 0, 0, 0],\n",
       "       [0, 0, 2, ..., 0, 0, 3],\n",
       "       [0, 0, 0, ..., 0, 2, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words(\"english\"))\n",
    "X = vectorizer.fit_transform([' '.join(einstein), ' '.join(newton), ' '.join(darwin), ' '.join(spielberg), ' '.join(allen), ' '.join(cameron)])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three biographies are scientists (class 0) while the last three are filmmakers (class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'aarau',\n",
       " u'abandon',\n",
       " u'abbey',\n",
       " u'abl',\n",
       " u'abraham',\n",
       " u'absenc',\n",
       " u'absorpt',\n",
       " u'abstract',\n",
       " u'absurd',\n",
       " u'abus']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2304)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work a smaller case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "Xv = vec.fit_transform(['apple soup soup', 'table stamp', 'donut king'])\n",
    "Xv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'apple', u'donut', u'king', u'soup', u'stamp', u'table']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# alpha is the smoothing parameter\n",
    "#clf = BernoulliNB(alpha=1.0, binarize=None, fit_prior=True, class_prior=None)\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check predictions on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = vectorizer.transform([' '.join(cameron), ' '.join(darwin)])\n",
    "clf.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 2, ..., 0, 0, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2304)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make prediction using data that the model has no seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kubrick = scrape_and_tokenize('Stanley_Kubrick')\n",
    "karle = scrape_and_tokenize('Jerome_Karle')\n",
    "obs = vectorizer.transform([' '.join(kubrick), ' '.join(karle)])\n",
    "clf.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model correctly predicts the classes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

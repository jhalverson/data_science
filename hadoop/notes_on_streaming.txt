The key for getting the streaming job to run was to specify a sub-directory
which does not exist within an existing directory.

hadoop-streaming -files s3://fridaybucket/mapper.py,s3://fridaybucket/reducer.py -mapper mapper.py -reducer reducer.py -input s3://fridaybucket/words.txt -output s3://streamingout/output

If one does not do this then you will get the error 'Output directory already exists.'

Note that I did not specify any arguments when submitting the job. The
output was three files (and an empty _SUCCESS file):

Sunday	1
Trees	1
ate	1
dog	2
in	1
made	1
on	1

BBQ	1
Cake	1
Frank	1
ants	1
are	1
big	1
by	1
first	1
grills	1
over	1
ran	1
the	2

Texas	1
a	1
hot	1
log	1
not	1
was	1

Here is a test to run locally:

cat words.txt | ./mapper.py | sort | ./reducer.py
